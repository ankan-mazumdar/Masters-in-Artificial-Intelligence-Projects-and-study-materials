{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY49iFDNs6ha"
      },
      "outputs": [],
      "source": [
        "# from _future_ import division\n",
        "# from _future_ import print_function\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import os  # Added import for os module\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# from pygcn.utils import load_data, accuracy\n",
        "# from pygcn.models import GCN\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from layers import GraphConvolution\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
        "                    enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "\n",
        "def load_data(path=\"/content/cora/\", dataset=\"cora\"):\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
        "                                        dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
        "                                    dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize(features)\n",
        "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Set the training settings\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.no_cuda = False  # Set to True if you want to disable CUDA\n",
        "        self.fastmode = False\n",
        "        self.seed = 42\n",
        "        self.epochs = 200\n",
        "        self.lr = 0.01\n",
        "        self.weight_decay = 5e-4\n",
        "        self.hidden = 16\n",
        "        self.dropout = 0.5\n",
        "\n",
        "args = Args()\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "# Specify the data directory\n",
        "data_dir = '/content/cora/'\n",
        "\n",
        "# Load data\n",
        "adj, features, labels, _, idx_val, idx_test = load_data(data_dir)\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=args.hidden,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=args.dropout)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "\n",
        "# Define a list of labeled node counts\n",
        "labeled_nodes_counts = [60, 120, 180, 240, 300]\n",
        "\n",
        "for labeled_nodes_count in labeled_nodes_counts:\n",
        "    # Randomly choose labeled nodes\n",
        "    idx_train = np.random.choice(idx_val, labeled_nodes_count, replace=False)\n",
        "\n",
        "    def train(epoch):\n",
        "        t = time.time()\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(features, adj)\n",
        "        loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "        acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if not args.fastmode:\n",
        "            # Evaluate validation set performance separately,\n",
        "            # deactivates dropout during the validation run.\n",
        "            model.eval()\n",
        "            output = model(features, adj)\n",
        "\n",
        "        loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "        acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "        print('Epoch: {:04d}'.format(epoch + 1),\n",
        "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "              'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "              'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "              'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "    def test():\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "        loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "        acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "        print(\"Test set results:\",\n",
        "              \"loss= {:.4f}\".format(loss_test.item()),\n",
        "              \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "    # Train model\n",
        "    t_total = time.time()\n",
        "    for epoch in range(args.epochs):\n",
        "        train(epoch)\n",
        "    print(f\"Optimization Finished for {labeled_nodes_count} labeled nodes!\")\n",
        "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "    # Testing\n",
        "    test()"
      ]
    }
  ]
}