{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
        "                    enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "\n",
        "def load_data(path=\"/content/cora/\", dataset=\"cora\"):\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
        "                                        dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
        "                                    dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize(features)\n",
        "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n"
      ],
      "metadata": {
        "id": "1we4pgPhGjJt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n"
      ],
      "metadata": {
        "id": "drFD-jEVG4JY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from layers import GraphConvolution\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "zRSylBHdG9I0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY49iFDNs6ha",
        "outputId": "2c6b2969-0bd1-4cae-f9fd-3fecb1f2c548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-7664072b6e0e>:99: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:605.)\n",
            "  return torch.sparse.FloatTensor(indices, values, shape)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 1.9001 acc_train: 0.2500 loss_val: 1.8905 acc_val: 0.3500 time: 0.0197s\n",
            "Epoch: 0002 loss_train: 1.8869 acc_train: 0.2667 loss_val: 1.8822 acc_val: 0.3500 time: 0.0194s\n",
            "Epoch: 0003 loss_train: 1.8758 acc_train: 0.2667 loss_val: 1.8748 acc_val: 0.3500 time: 0.0191s\n",
            "Epoch: 0004 loss_train: 1.8513 acc_train: 0.2667 loss_val: 1.8677 acc_val: 0.3500 time: 0.0185s\n",
            "Epoch: 0005 loss_train: 1.8639 acc_train: 0.2667 loss_val: 1.8613 acc_val: 0.3500 time: 0.0201s\n",
            "Epoch: 0006 loss_train: 1.8515 acc_train: 0.2667 loss_val: 1.8550 acc_val: 0.3500 time: 0.0188s\n",
            "Epoch: 0007 loss_train: 1.8421 acc_train: 0.2667 loss_val: 1.8496 acc_val: 0.3500 time: 0.0179s\n",
            "Epoch: 0008 loss_train: 1.8457 acc_train: 0.2667 loss_val: 1.8445 acc_val: 0.3500 time: 0.0205s\n",
            "Epoch: 0009 loss_train: 1.8344 acc_train: 0.2667 loss_val: 1.8396 acc_val: 0.3500 time: 0.0185s\n",
            "Epoch: 0010 loss_train: 1.8232 acc_train: 0.2667 loss_val: 1.8351 acc_val: 0.3500 time: 0.0192s\n",
            "Epoch: 0011 loss_train: 1.8218 acc_train: 0.2667 loss_val: 1.8308 acc_val: 0.3500 time: 0.0226s\n",
            "Epoch: 0012 loss_train: 1.8173 acc_train: 0.2667 loss_val: 1.8272 acc_val: 0.3500 time: 0.0178s\n",
            "Epoch: 0013 loss_train: 1.8142 acc_train: 0.2667 loss_val: 1.8239 acc_val: 0.3500 time: 0.0195s\n",
            "Epoch: 0014 loss_train: 1.8125 acc_train: 0.2833 loss_val: 1.8207 acc_val: 0.3500 time: 0.0180s\n",
            "Epoch: 0015 loss_train: 1.7990 acc_train: 0.2833 loss_val: 1.8175 acc_val: 0.3500 time: 0.0182s\n",
            "Epoch: 0016 loss_train: 1.7963 acc_train: 0.2667 loss_val: 1.8145 acc_val: 0.3500 time: 0.0198s\n",
            "Epoch: 0017 loss_train: 1.7794 acc_train: 0.2833 loss_val: 1.8116 acc_val: 0.3500 time: 0.0187s\n",
            "Epoch: 0018 loss_train: 1.7874 acc_train: 0.2667 loss_val: 1.8086 acc_val: 0.3500 time: 0.0176s\n",
            "Epoch: 0019 loss_train: 1.7767 acc_train: 0.3000 loss_val: 1.8061 acc_val: 0.3500 time: 0.0176s\n",
            "Epoch: 0020 loss_train: 1.7805 acc_train: 0.2833 loss_val: 1.8036 acc_val: 0.3500 time: 0.0181s\n",
            "Epoch: 0021 loss_train: 1.7479 acc_train: 0.2833 loss_val: 1.8010 acc_val: 0.3500 time: 0.0212s\n",
            "Epoch: 0022 loss_train: 1.7465 acc_train: 0.2833 loss_val: 1.7984 acc_val: 0.3500 time: 0.0179s\n",
            "Epoch: 0023 loss_train: 1.7461 acc_train: 0.3000 loss_val: 1.7959 acc_val: 0.3500 time: 0.0177s\n",
            "Epoch: 0024 loss_train: 1.7391 acc_train: 0.2667 loss_val: 1.7928 acc_val: 0.3500 time: 0.0183s\n",
            "Epoch: 0025 loss_train: 1.7319 acc_train: 0.3000 loss_val: 1.7892 acc_val: 0.3500 time: 0.0183s\n",
            "Epoch: 0026 loss_train: 1.7200 acc_train: 0.3000 loss_val: 1.7850 acc_val: 0.3500 time: 0.0204s\n",
            "Epoch: 0027 loss_train: 1.7122 acc_train: 0.3000 loss_val: 1.7805 acc_val: 0.3533 time: 0.0176s\n",
            "Epoch: 0028 loss_train: 1.6983 acc_train: 0.2833 loss_val: 1.7755 acc_val: 0.3533 time: 0.0180s\n",
            "Epoch: 0029 loss_train: 1.6712 acc_train: 0.3167 loss_val: 1.7701 acc_val: 0.3533 time: 0.0178s\n",
            "Epoch: 0030 loss_train: 1.6707 acc_train: 0.3667 loss_val: 1.7643 acc_val: 0.3600 time: 0.0181s\n",
            "Epoch: 0031 loss_train: 1.6618 acc_train: 0.3000 loss_val: 1.7583 acc_val: 0.3600 time: 0.0214s\n",
            "Epoch: 0032 loss_train: 1.6656 acc_train: 0.2833 loss_val: 1.7522 acc_val: 0.3600 time: 0.0181s\n",
            "Epoch: 0033 loss_train: 1.6505 acc_train: 0.3000 loss_val: 1.7460 acc_val: 0.3700 time: 0.0178s\n",
            "Epoch: 0034 loss_train: 1.6519 acc_train: 0.3667 loss_val: 1.7395 acc_val: 0.3833 time: 0.0200s\n",
            "Epoch: 0035 loss_train: 1.6149 acc_train: 0.3667 loss_val: 1.7325 acc_val: 0.3933 time: 0.0172s\n",
            "Epoch: 0036 loss_train: 1.5898 acc_train: 0.4000 loss_val: 1.7256 acc_val: 0.3967 time: 0.0203s\n",
            "Epoch: 0037 loss_train: 1.6065 acc_train: 0.3833 loss_val: 1.7182 acc_val: 0.3967 time: 0.0175s\n",
            "Epoch: 0038 loss_train: 1.5945 acc_train: 0.3500 loss_val: 1.7106 acc_val: 0.4100 time: 0.0179s\n",
            "Epoch: 0039 loss_train: 1.5775 acc_train: 0.4333 loss_val: 1.7027 acc_val: 0.4300 time: 0.0173s\n",
            "Epoch: 0040 loss_train: 1.5563 acc_train: 0.4167 loss_val: 1.6937 acc_val: 0.4367 time: 0.0192s\n",
            "Epoch: 0041 loss_train: 1.5361 acc_train: 0.4000 loss_val: 1.6841 acc_val: 0.4433 time: 0.0313s\n",
            "Epoch: 0042 loss_train: 1.5363 acc_train: 0.4667 loss_val: 1.6737 acc_val: 0.4567 time: 0.0238s\n",
            "Epoch: 0043 loss_train: 1.4965 acc_train: 0.4333 loss_val: 1.6634 acc_val: 0.4533 time: 0.0182s\n",
            "Epoch: 0044 loss_train: 1.4841 acc_train: 0.4500 loss_val: 1.6530 acc_val: 0.4567 time: 0.0173s\n",
            "Epoch: 0045 loss_train: 1.4916 acc_train: 0.4833 loss_val: 1.6424 acc_val: 0.4667 time: 0.0173s\n",
            "Epoch: 0046 loss_train: 1.4322 acc_train: 0.5167 loss_val: 1.6320 acc_val: 0.4733 time: 0.0172s\n",
            "Epoch: 0047 loss_train: 1.4652 acc_train: 0.4833 loss_val: 1.6215 acc_val: 0.4867 time: 0.0184s\n",
            "Epoch: 0048 loss_train: 1.4500 acc_train: 0.5000 loss_val: 1.6115 acc_val: 0.5033 time: 0.0185s\n",
            "Epoch: 0049 loss_train: 1.4056 acc_train: 0.5833 loss_val: 1.6013 acc_val: 0.5133 time: 0.0183s\n",
            "Epoch: 0050 loss_train: 1.4351 acc_train: 0.5333 loss_val: 1.5912 acc_val: 0.5200 time: 0.0185s\n",
            "Epoch: 0051 loss_train: 1.3505 acc_train: 0.5667 loss_val: 1.5804 acc_val: 0.5300 time: 0.0221s\n",
            "Epoch: 0052 loss_train: 1.3281 acc_train: 0.6667 loss_val: 1.5689 acc_val: 0.5367 time: 0.0191s\n",
            "Epoch: 0053 loss_train: 1.3308 acc_train: 0.6667 loss_val: 1.5572 acc_val: 0.5367 time: 0.0178s\n",
            "Epoch: 0054 loss_train: 1.3495 acc_train: 0.6667 loss_val: 1.5442 acc_val: 0.5400 time: 0.0182s\n",
            "Epoch: 0055 loss_train: 1.3347 acc_train: 0.6333 loss_val: 1.5316 acc_val: 0.5467 time: 0.0174s\n",
            "Epoch: 0056 loss_train: 1.3210 acc_train: 0.5833 loss_val: 1.5188 acc_val: 0.5500 time: 0.0171s\n",
            "Epoch: 0057 loss_train: 1.2576 acc_train: 0.6333 loss_val: 1.5067 acc_val: 0.5667 time: 0.0203s\n",
            "Epoch: 0058 loss_train: 1.2866 acc_train: 0.6333 loss_val: 1.4946 acc_val: 0.5767 time: 0.0178s\n",
            "Epoch: 0059 loss_train: 1.2720 acc_train: 0.6833 loss_val: 1.4823 acc_val: 0.5867 time: 0.0178s\n",
            "Epoch: 0060 loss_train: 1.2262 acc_train: 0.6667 loss_val: 1.4704 acc_val: 0.5933 time: 0.0178s\n",
            "Epoch: 0061 loss_train: 1.2116 acc_train: 0.6833 loss_val: 1.4589 acc_val: 0.6000 time: 0.0206s\n",
            "Epoch: 0062 loss_train: 1.1750 acc_train: 0.7333 loss_val: 1.4469 acc_val: 0.6000 time: 0.0171s\n",
            "Epoch: 0063 loss_train: 1.1543 acc_train: 0.7333 loss_val: 1.4352 acc_val: 0.6100 time: 0.0178s\n",
            "Epoch: 0064 loss_train: 1.1614 acc_train: 0.7167 loss_val: 1.4246 acc_val: 0.6100 time: 0.0184s\n",
            "Epoch: 0065 loss_train: 1.1345 acc_train: 0.7333 loss_val: 1.4152 acc_val: 0.6133 time: 0.0176s\n",
            "Epoch: 0066 loss_train: 1.0966 acc_train: 0.8000 loss_val: 1.4059 acc_val: 0.6200 time: 0.0215s\n",
            "Epoch: 0067 loss_train: 1.1295 acc_train: 0.7500 loss_val: 1.3971 acc_val: 0.6233 time: 0.0181s\n",
            "Epoch: 0068 loss_train: 1.1093 acc_train: 0.7833 loss_val: 1.3878 acc_val: 0.6367 time: 0.0170s\n",
            "Epoch: 0069 loss_train: 1.1127 acc_train: 0.7333 loss_val: 1.3787 acc_val: 0.6400 time: 0.0177s\n",
            "Epoch: 0070 loss_train: 1.0691 acc_train: 0.7833 loss_val: 1.3698 acc_val: 0.6433 time: 0.0180s\n",
            "Epoch: 0071 loss_train: 1.0140 acc_train: 0.8167 loss_val: 1.3598 acc_val: 0.6467 time: 0.0212s\n",
            "Epoch: 0072 loss_train: 1.0084 acc_train: 0.7833 loss_val: 1.3481 acc_val: 0.6467 time: 0.0187s\n",
            "Epoch: 0073 loss_train: 1.0566 acc_train: 0.7333 loss_val: 1.3353 acc_val: 0.6467 time: 0.0169s\n",
            "Epoch: 0074 loss_train: 1.0269 acc_train: 0.8500 loss_val: 1.3217 acc_val: 0.6467 time: 0.0179s\n",
            "Epoch: 0075 loss_train: 0.9533 acc_train: 0.8500 loss_val: 1.3083 acc_val: 0.6500 time: 0.0183s\n",
            "Epoch: 0076 loss_train: 0.9517 acc_train: 0.7833 loss_val: 1.2967 acc_val: 0.6500 time: 0.0180s\n",
            "Epoch: 0077 loss_train: 0.9937 acc_train: 0.8000 loss_val: 1.2864 acc_val: 0.6500 time: 0.0217s\n",
            "Epoch: 0078 loss_train: 0.9376 acc_train: 0.7667 loss_val: 1.2777 acc_val: 0.6600 time: 0.0174s\n",
            "Epoch: 0079 loss_train: 0.9579 acc_train: 0.8333 loss_val: 1.2698 acc_val: 0.6633 time: 0.0179s\n",
            "Epoch: 0080 loss_train: 0.9802 acc_train: 0.7833 loss_val: 1.2622 acc_val: 0.6733 time: 0.0181s\n",
            "Epoch: 0081 loss_train: 0.9328 acc_train: 0.8000 loss_val: 1.2544 acc_val: 0.6800 time: 0.0205s\n",
            "Epoch: 0082 loss_train: 0.9382 acc_train: 0.7667 loss_val: 1.2467 acc_val: 0.6900 time: 0.0177s\n",
            "Epoch: 0083 loss_train: 0.9035 acc_train: 0.7833 loss_val: 1.2405 acc_val: 0.6933 time: 0.0172s\n",
            "Epoch: 0084 loss_train: 0.8874 acc_train: 0.8333 loss_val: 1.2322 acc_val: 0.6933 time: 0.0179s\n",
            "Epoch: 0085 loss_train: 0.8907 acc_train: 0.8500 loss_val: 1.2227 acc_val: 0.7000 time: 0.0184s\n",
            "Epoch: 0086 loss_train: 0.8844 acc_train: 0.8000 loss_val: 1.2114 acc_val: 0.6933 time: 0.0176s\n",
            "Epoch: 0087 loss_train: 0.8859 acc_train: 0.8500 loss_val: 1.1994 acc_val: 0.6900 time: 0.0201s\n",
            "Epoch: 0088 loss_train: 0.8903 acc_train: 0.8333 loss_val: 1.1879 acc_val: 0.6900 time: 0.0175s\n",
            "Epoch: 0089 loss_train: 0.8001 acc_train: 0.8500 loss_val: 1.1775 acc_val: 0.6867 time: 0.0178s\n",
            "Epoch: 0090 loss_train: 0.7977 acc_train: 0.8500 loss_val: 1.1686 acc_val: 0.6900 time: 0.0218s\n",
            "Epoch: 0091 loss_train: 0.8550 acc_train: 0.8167 loss_val: 1.1595 acc_val: 0.6933 time: 0.0245s\n",
            "Epoch: 0092 loss_train: 0.8383 acc_train: 0.8167 loss_val: 1.1516 acc_val: 0.7000 time: 0.0179s\n",
            "Epoch: 0093 loss_train: 0.8192 acc_train: 0.9000 loss_val: 1.1448 acc_val: 0.7033 time: 0.0188s\n",
            "Epoch: 0094 loss_train: 0.7544 acc_train: 0.8667 loss_val: 1.1383 acc_val: 0.7133 time: 0.0208s\n",
            "Epoch: 0095 loss_train: 0.7855 acc_train: 0.8500 loss_val: 1.1320 acc_val: 0.7133 time: 0.0183s\n",
            "Epoch: 0096 loss_train: 0.8153 acc_train: 0.7833 loss_val: 1.1260 acc_val: 0.7100 time: 0.0197s\n",
            "Epoch: 0097 loss_train: 0.8253 acc_train: 0.8333 loss_val: 1.1195 acc_val: 0.7233 time: 0.0174s\n",
            "Epoch: 0098 loss_train: 0.7277 acc_train: 0.8833 loss_val: 1.1138 acc_val: 0.7200 time: 0.0177s\n",
            "Epoch: 0099 loss_train: 0.7739 acc_train: 0.9000 loss_val: 1.1089 acc_val: 0.7233 time: 0.0190s\n",
            "Epoch: 0100 loss_train: 0.8059 acc_train: 0.8333 loss_val: 1.1039 acc_val: 0.7233 time: 0.0182s\n",
            "Epoch: 0101 loss_train: 0.7713 acc_train: 0.8667 loss_val: 1.0993 acc_val: 0.7300 time: 0.0210s\n",
            "Epoch: 0102 loss_train: 0.7448 acc_train: 0.8500 loss_val: 1.0934 acc_val: 0.7200 time: 0.0181s\n",
            "Epoch: 0103 loss_train: 0.7461 acc_train: 0.8333 loss_val: 1.0872 acc_val: 0.7167 time: 0.0181s\n",
            "Epoch: 0104 loss_train: 0.7364 acc_train: 0.8667 loss_val: 1.0817 acc_val: 0.7233 time: 0.0219s\n",
            "Epoch: 0105 loss_train: 0.6953 acc_train: 0.9000 loss_val: 1.0765 acc_val: 0.7267 time: 0.0177s\n",
            "Epoch: 0106 loss_train: 0.7060 acc_train: 0.8500 loss_val: 1.0717 acc_val: 0.7267 time: 0.0211s\n",
            "Epoch: 0107 loss_train: 0.7098 acc_train: 0.8500 loss_val: 1.0664 acc_val: 0.7267 time: 0.0191s\n",
            "Epoch: 0108 loss_train: 0.6389 acc_train: 0.8833 loss_val: 1.0596 acc_val: 0.7233 time: 0.0189s\n",
            "Epoch: 0109 loss_train: 0.7452 acc_train: 0.8500 loss_val: 1.0528 acc_val: 0.7233 time: 0.0180s\n",
            "Epoch: 0110 loss_train: 0.6824 acc_train: 0.9000 loss_val: 1.0453 acc_val: 0.7233 time: 0.0184s\n",
            "Epoch: 0111 loss_train: 0.6763 acc_train: 0.9167 loss_val: 1.0384 acc_val: 0.7200 time: 0.0239s\n",
            "Epoch: 0112 loss_train: 0.6433 acc_train: 0.8667 loss_val: 1.0319 acc_val: 0.7267 time: 0.0198s\n",
            "Epoch: 0113 loss_train: 0.6130 acc_train: 0.9000 loss_val: 1.0265 acc_val: 0.7233 time: 0.0186s\n",
            "Epoch: 0114 loss_train: 0.7143 acc_train: 0.8667 loss_val: 1.0220 acc_val: 0.7267 time: 0.0179s\n",
            "Epoch: 0115 loss_train: 0.6767 acc_train: 0.8833 loss_val: 1.0185 acc_val: 0.7300 time: 0.0183s\n",
            "Epoch: 0116 loss_train: 0.6784 acc_train: 0.8500 loss_val: 1.0154 acc_val: 0.7333 time: 0.0200s\n",
            "Epoch: 0117 loss_train: 0.6649 acc_train: 0.9167 loss_val: 1.0109 acc_val: 0.7300 time: 0.0186s\n",
            "Epoch: 0118 loss_train: 0.6653 acc_train: 0.9167 loss_val: 1.0054 acc_val: 0.7300 time: 0.0184s\n",
            "Epoch: 0119 loss_train: 0.6311 acc_train: 0.8833 loss_val: 1.0001 acc_val: 0.7300 time: 0.0184s\n",
            "Epoch: 0120 loss_train: 0.6479 acc_train: 0.9333 loss_val: 0.9945 acc_val: 0.7333 time: 0.0189s\n",
            "Epoch: 0121 loss_train: 0.6269 acc_train: 0.8667 loss_val: 0.9909 acc_val: 0.7333 time: 0.0233s\n",
            "Epoch: 0122 loss_train: 0.6919 acc_train: 0.8833 loss_val: 0.9877 acc_val: 0.7300 time: 0.0190s\n",
            "Epoch: 0123 loss_train: 0.6148 acc_train: 0.9000 loss_val: 0.9846 acc_val: 0.7233 time: 0.0199s\n",
            "Epoch: 0124 loss_train: 0.6496 acc_train: 0.8667 loss_val: 0.9806 acc_val: 0.7300 time: 0.0177s\n",
            "Epoch: 0125 loss_train: 0.6967 acc_train: 0.8667 loss_val: 0.9755 acc_val: 0.7300 time: 0.0172s\n",
            "Epoch: 0126 loss_train: 0.6289 acc_train: 0.8500 loss_val: 0.9703 acc_val: 0.7300 time: 0.0199s\n",
            "Epoch: 0127 loss_train: 0.6612 acc_train: 0.8667 loss_val: 0.9648 acc_val: 0.7267 time: 0.0172s\n",
            "Epoch: 0128 loss_train: 0.5434 acc_train: 0.9000 loss_val: 0.9584 acc_val: 0.7233 time: 0.0175s\n",
            "Epoch: 0129 loss_train: 0.6084 acc_train: 0.9000 loss_val: 0.9519 acc_val: 0.7300 time: 0.0177s\n",
            "Epoch: 0130 loss_train: 0.6248 acc_train: 0.8500 loss_val: 0.9468 acc_val: 0.7267 time: 0.0172s\n",
            "Epoch: 0131 loss_train: 0.5777 acc_train: 0.8500 loss_val: 0.9429 acc_val: 0.7200 time: 0.0203s\n",
            "Epoch: 0132 loss_train: 0.5623 acc_train: 0.9167 loss_val: 0.9388 acc_val: 0.7200 time: 0.0193s\n",
            "Epoch: 0133 loss_train: 0.5966 acc_train: 0.8667 loss_val: 0.9358 acc_val: 0.7200 time: 0.0183s\n",
            "Epoch: 0134 loss_train: 0.5788 acc_train: 0.8500 loss_val: 0.9318 acc_val: 0.7200 time: 0.0180s\n",
            "Epoch: 0135 loss_train: 0.5364 acc_train: 0.8833 loss_val: 0.9279 acc_val: 0.7267 time: 0.0177s\n",
            "Epoch: 0136 loss_train: 0.5650 acc_train: 0.8833 loss_val: 0.9250 acc_val: 0.7267 time: 0.0175s\n",
            "Epoch: 0137 loss_train: 0.5598 acc_train: 0.9167 loss_val: 0.9228 acc_val: 0.7267 time: 0.0229s\n",
            "Epoch: 0138 loss_train: 0.5288 acc_train: 0.9000 loss_val: 0.9207 acc_val: 0.7233 time: 0.0216s\n",
            "Epoch: 0139 loss_train: 0.5836 acc_train: 0.8167 loss_val: 0.9182 acc_val: 0.7367 time: 0.0169s\n",
            "Epoch: 0140 loss_train: 0.5785 acc_train: 0.8833 loss_val: 0.9160 acc_val: 0.7333 time: 0.0184s\n",
            "Epoch: 0141 loss_train: 0.5842 acc_train: 0.9333 loss_val: 0.9147 acc_val: 0.7333 time: 0.0215s\n",
            "Epoch: 0142 loss_train: 0.5958 acc_train: 0.8667 loss_val: 0.9140 acc_val: 0.7333 time: 0.0181s\n",
            "Epoch: 0143 loss_train: 0.5909 acc_train: 0.9167 loss_val: 0.9116 acc_val: 0.7300 time: 0.0192s\n",
            "Epoch: 0144 loss_train: 0.4991 acc_train: 0.9000 loss_val: 0.9063 acc_val: 0.7267 time: 0.0184s\n",
            "Epoch: 0145 loss_train: 0.5646 acc_train: 0.9000 loss_val: 0.9008 acc_val: 0.7267 time: 0.0280s\n",
            "Epoch: 0146 loss_train: 0.5050 acc_train: 0.9333 loss_val: 0.8951 acc_val: 0.7233 time: 0.0176s\n",
            "Epoch: 0147 loss_train: 0.5397 acc_train: 0.8833 loss_val: 0.8899 acc_val: 0.7200 time: 0.0168s\n",
            "Epoch: 0148 loss_train: 0.5265 acc_train: 0.9000 loss_val: 0.8854 acc_val: 0.7267 time: 0.0164s\n",
            "Epoch: 0149 loss_train: 0.5249 acc_train: 0.9167 loss_val: 0.8832 acc_val: 0.7300 time: 0.0173s\n",
            "Epoch: 0150 loss_train: 0.5524 acc_train: 0.8833 loss_val: 0.8815 acc_val: 0.7333 time: 0.0173s\n",
            "Epoch: 0151 loss_train: 0.5684 acc_train: 0.9167 loss_val: 0.8796 acc_val: 0.7400 time: 0.0244s\n",
            "Epoch: 0152 loss_train: 0.5305 acc_train: 0.9333 loss_val: 0.8774 acc_val: 0.7400 time: 0.0179s\n",
            "Epoch: 0153 loss_train: 0.4897 acc_train: 0.9167 loss_val: 0.8753 acc_val: 0.7433 time: 0.0177s\n",
            "Epoch: 0154 loss_train: 0.5195 acc_train: 0.9000 loss_val: 0.8733 acc_val: 0.7400 time: 0.0174s\n",
            "Epoch: 0155 loss_train: 0.5193 acc_train: 0.9333 loss_val: 0.8710 acc_val: 0.7400 time: 0.0179s\n",
            "Epoch: 0156 loss_train: 0.5454 acc_train: 0.9000 loss_val: 0.8685 acc_val: 0.7400 time: 0.0182s\n",
            "Epoch: 0157 loss_train: 0.5473 acc_train: 0.9000 loss_val: 0.8647 acc_val: 0.7367 time: 0.0191s\n",
            "Epoch: 0158 loss_train: 0.4991 acc_train: 0.9000 loss_val: 0.8613 acc_val: 0.7367 time: 0.0180s\n",
            "Epoch: 0159 loss_train: 0.4795 acc_train: 0.9333 loss_val: 0.8578 acc_val: 0.7267 time: 0.0201s\n",
            "Epoch: 0160 loss_train: 0.4891 acc_train: 0.9167 loss_val: 0.8536 acc_val: 0.7267 time: 0.0207s\n",
            "Epoch: 0161 loss_train: 0.4903 acc_train: 0.9167 loss_val: 0.8487 acc_val: 0.7267 time: 0.0255s\n",
            "Epoch: 0162 loss_train: 0.4761 acc_train: 0.9167 loss_val: 0.8446 acc_val: 0.7300 time: 0.0186s\n",
            "Epoch: 0163 loss_train: 0.4668 acc_train: 0.9167 loss_val: 0.8394 acc_val: 0.7300 time: 0.0182s\n",
            "Epoch: 0164 loss_train: 0.4291 acc_train: 0.9833 loss_val: 0.8345 acc_val: 0.7367 time: 0.0182s\n",
            "Epoch: 0165 loss_train: 0.4646 acc_train: 0.9667 loss_val: 0.8315 acc_val: 0.7300 time: 0.0204s\n",
            "Epoch: 0166 loss_train: 0.4143 acc_train: 0.9500 loss_val: 0.8285 acc_val: 0.7333 time: 0.0183s\n",
            "Epoch: 0167 loss_train: 0.4828 acc_train: 0.9333 loss_val: 0.8262 acc_val: 0.7267 time: 0.0182s\n",
            "Epoch: 0168 loss_train: 0.4359 acc_train: 0.9500 loss_val: 0.8238 acc_val: 0.7300 time: 0.0203s\n",
            "Epoch: 0169 loss_train: 0.4642 acc_train: 0.9000 loss_val: 0.8210 acc_val: 0.7300 time: 0.0180s\n",
            "Epoch: 0170 loss_train: 0.4708 acc_train: 0.9000 loss_val: 0.8188 acc_val: 0.7367 time: 0.0215s\n",
            "Epoch: 0171 loss_train: 0.4661 acc_train: 0.9333 loss_val: 0.8168 acc_val: 0.7367 time: 0.0181s\n",
            "Epoch: 0172 loss_train: 0.4608 acc_train: 0.9333 loss_val: 0.8148 acc_val: 0.7367 time: 0.0188s\n",
            "Epoch: 0173 loss_train: 0.3898 acc_train: 0.9500 loss_val: 0.8130 acc_val: 0.7433 time: 0.0178s\n",
            "Epoch: 0174 loss_train: 0.4500 acc_train: 0.9000 loss_val: 0.8093 acc_val: 0.7433 time: 0.0174s\n",
            "Epoch: 0175 loss_train: 0.4578 acc_train: 0.9167 loss_val: 0.8053 acc_val: 0.7433 time: 0.0174s\n",
            "Epoch: 0176 loss_train: 0.4027 acc_train: 0.9167 loss_val: 0.8006 acc_val: 0.7467 time: 0.0191s\n",
            "Epoch: 0177 loss_train: 0.4842 acc_train: 0.9167 loss_val: 0.7957 acc_val: 0.7400 time: 0.0174s\n",
            "Epoch: 0178 loss_train: 0.4990 acc_train: 0.9167 loss_val: 0.7922 acc_val: 0.7400 time: 0.0177s\n",
            "Epoch: 0179 loss_train: 0.4331 acc_train: 0.9333 loss_val: 0.7895 acc_val: 0.7400 time: 0.0180s\n",
            "Epoch: 0180 loss_train: 0.4242 acc_train: 0.9000 loss_val: 0.7869 acc_val: 0.7400 time: 0.0230s\n",
            "Epoch: 0181 loss_train: 0.4220 acc_train: 0.9500 loss_val: 0.7843 acc_val: 0.7433 time: 0.0183s\n",
            "Epoch: 0182 loss_train: 0.3593 acc_train: 0.9333 loss_val: 0.7820 acc_val: 0.7400 time: 0.0187s\n",
            "Epoch: 0183 loss_train: 0.4536 acc_train: 0.9167 loss_val: 0.7810 acc_val: 0.7467 time: 0.0179s\n",
            "Epoch: 0184 loss_train: 0.4718 acc_train: 0.9167 loss_val: 0.7801 acc_val: 0.7467 time: 0.0317s\n",
            "Epoch: 0185 loss_train: 0.3840 acc_train: 0.9333 loss_val: 0.7783 acc_val: 0.7433 time: 0.0211s\n",
            "Epoch: 0186 loss_train: 0.4209 acc_train: 0.9167 loss_val: 0.7764 acc_val: 0.7467 time: 0.0186s\n",
            "Epoch: 0187 loss_train: 0.4371 acc_train: 0.9500 loss_val: 0.7740 acc_val: 0.7500 time: 0.0209s\n",
            "Epoch: 0188 loss_train: 0.3708 acc_train: 0.9333 loss_val: 0.7720 acc_val: 0.7500 time: 0.0228s\n",
            "Epoch: 0189 loss_train: 0.3559 acc_train: 0.9333 loss_val: 0.7698 acc_val: 0.7467 time: 0.0206s\n",
            "Epoch: 0190 loss_train: 0.4133 acc_train: 0.9000 loss_val: 0.7666 acc_val: 0.7467 time: 0.0192s\n",
            "Epoch: 0191 loss_train: 0.4074 acc_train: 0.9333 loss_val: 0.7622 acc_val: 0.7500 time: 0.0209s\n",
            "Epoch: 0192 loss_train: 0.4298 acc_train: 0.9333 loss_val: 0.7568 acc_val: 0.7567 time: 0.0177s\n",
            "Epoch: 0193 loss_train: 0.4058 acc_train: 0.9500 loss_val: 0.7513 acc_val: 0.7467 time: 0.0179s\n",
            "Epoch: 0194 loss_train: 0.4123 acc_train: 0.9500 loss_val: 0.7466 acc_val: 0.7467 time: 0.0204s\n",
            "Epoch: 0195 loss_train: 0.4577 acc_train: 0.9500 loss_val: 0.7426 acc_val: 0.7500 time: 0.0208s\n",
            "Epoch: 0196 loss_train: 0.3879 acc_train: 0.9333 loss_val: 0.7398 acc_val: 0.7533 time: 0.0176s\n",
            "Epoch: 0197 loss_train: 0.3921 acc_train: 0.9500 loss_val: 0.7370 acc_val: 0.7600 time: 0.0181s\n",
            "Epoch: 0198 loss_train: 0.3498 acc_train: 0.9667 loss_val: 0.7354 acc_val: 0.7567 time: 0.0180s\n",
            "Epoch: 0199 loss_train: 0.4020 acc_train: 0.9000 loss_val: 0.7347 acc_val: 0.7633 time: 0.0202s\n",
            "Epoch: 0200 loss_train: 0.3628 acc_train: 0.9500 loss_val: 0.7334 acc_val: 0.7600 time: 0.0195s\n",
            "Optimization Finished for 60 labeled nodes!\n",
            "Total time elapsed: 4.3536s\n",
            "Test set results: loss= 0.8275 accuracy= 0.7400\n",
            "Epoch: 0001 loss_train: 0.8169 acc_train: 0.7083 loss_val: 0.7200 acc_val: 0.7867 time: 0.0245s\n",
            "Epoch: 0002 loss_train: 0.8694 acc_train: 0.7000 loss_val: 0.7024 acc_val: 0.7933 time: 0.0256s\n",
            "Epoch: 0003 loss_train: 0.8377 acc_train: 0.7750 loss_val: 0.6851 acc_val: 0.8167 time: 0.0276s\n",
            "Epoch: 0004 loss_train: 0.7828 acc_train: 0.7833 loss_val: 0.6727 acc_val: 0.8333 time: 0.0254s\n",
            "Epoch: 0005 loss_train: 0.8635 acc_train: 0.7750 loss_val: 0.6677 acc_val: 0.8400 time: 0.0258s\n",
            "Epoch: 0006 loss_train: 0.7095 acc_train: 0.8083 loss_val: 0.6654 acc_val: 0.8467 time: 0.0257s\n",
            "Epoch: 0007 loss_train: 0.6497 acc_train: 0.8250 loss_val: 0.6644 acc_val: 0.8500 time: 0.0265s\n",
            "Epoch: 0008 loss_train: 0.7472 acc_train: 0.8167 loss_val: 0.6599 acc_val: 0.8467 time: 0.0270s\n",
            "Epoch: 0009 loss_train: 0.7257 acc_train: 0.8333 loss_val: 0.6528 acc_val: 0.8333 time: 0.0259s\n",
            "Epoch: 0010 loss_train: 0.6882 acc_train: 0.8583 loss_val: 0.6422 acc_val: 0.8333 time: 0.0479s\n",
            "Epoch: 0011 loss_train: 0.6457 acc_train: 0.8667 loss_val: 0.6283 acc_val: 0.8367 time: 0.0277s\n",
            "Epoch: 0012 loss_train: 0.7011 acc_train: 0.8667 loss_val: 0.6144 acc_val: 0.8333 time: 0.0250s\n",
            "Epoch: 0013 loss_train: 0.5876 acc_train: 0.8917 loss_val: 0.6012 acc_val: 0.8367 time: 0.0245s\n",
            "Epoch: 0014 loss_train: 0.4996 acc_train: 0.9000 loss_val: 0.5887 acc_val: 0.8467 time: 0.0308s\n",
            "Epoch: 0015 loss_train: 0.5594 acc_train: 0.8583 loss_val: 0.5765 acc_val: 0.8533 time: 0.0239s\n",
            "Epoch: 0016 loss_train: 0.4856 acc_train: 0.9083 loss_val: 0.5645 acc_val: 0.8633 time: 0.0269s\n",
            "Epoch: 0017 loss_train: 0.5103 acc_train: 0.8833 loss_val: 0.5532 acc_val: 0.8733 time: 0.0256s\n",
            "Epoch: 0018 loss_train: 0.5041 acc_train: 0.8917 loss_val: 0.5426 acc_val: 0.8767 time: 0.0257s\n",
            "Epoch: 0019 loss_train: 0.5250 acc_train: 0.8667 loss_val: 0.5338 acc_val: 0.8800 time: 0.0250s\n",
            "Epoch: 0020 loss_train: 0.4820 acc_train: 0.9083 loss_val: 0.5263 acc_val: 0.8833 time: 0.0242s\n",
            "Epoch: 0021 loss_train: 0.4971 acc_train: 0.9167 loss_val: 0.5218 acc_val: 0.8867 time: 0.0247s\n",
            "Epoch: 0022 loss_train: 0.4956 acc_train: 0.9083 loss_val: 0.5207 acc_val: 0.8833 time: 0.0307s\n",
            "Epoch: 0023 loss_train: 0.4843 acc_train: 0.8917 loss_val: 0.5227 acc_val: 0.8733 time: 0.0248s\n",
            "Epoch: 0024 loss_train: 0.4622 acc_train: 0.8833 loss_val: 0.5245 acc_val: 0.8700 time: 0.0244s\n",
            "Epoch: 0025 loss_train: 0.4832 acc_train: 0.9250 loss_val: 0.5269 acc_val: 0.8667 time: 0.0246s\n",
            "Epoch: 0026 loss_train: 0.4585 acc_train: 0.9083 loss_val: 0.5302 acc_val: 0.8633 time: 0.0264s\n",
            "Epoch: 0027 loss_train: 0.4141 acc_train: 0.9333 loss_val: 0.5336 acc_val: 0.8667 time: 0.0240s\n",
            "Epoch: 0028 loss_train: 0.4533 acc_train: 0.9250 loss_val: 0.5364 acc_val: 0.8633 time: 0.0241s\n",
            "Epoch: 0029 loss_train: 0.4330 acc_train: 0.9667 loss_val: 0.5385 acc_val: 0.8600 time: 0.0249s\n",
            "Epoch: 0030 loss_train: 0.4013 acc_train: 0.9167 loss_val: 0.5400 acc_val: 0.8567 time: 0.0305s\n",
            "Epoch: 0031 loss_train: 0.4099 acc_train: 0.9250 loss_val: 0.5405 acc_val: 0.8567 time: 0.0268s\n",
            "Epoch: 0032 loss_train: 0.3975 acc_train: 0.9000 loss_val: 0.5391 acc_val: 0.8600 time: 0.0275s\n",
            "Epoch: 0033 loss_train: 0.3864 acc_train: 0.9500 loss_val: 0.5382 acc_val: 0.8567 time: 0.0243s\n",
            "Epoch: 0034 loss_train: 0.3965 acc_train: 0.9417 loss_val: 0.5365 acc_val: 0.8533 time: 0.0245s\n",
            "Epoch: 0035 loss_train: 0.4337 acc_train: 0.9333 loss_val: 0.5343 acc_val: 0.8600 time: 0.0247s\n",
            "Epoch: 0036 loss_train: 0.3758 acc_train: 0.9333 loss_val: 0.5332 acc_val: 0.8633 time: 0.0250s\n",
            "Epoch: 0037 loss_train: 0.3865 acc_train: 0.9500 loss_val: 0.5321 acc_val: 0.8700 time: 0.0247s\n",
            "Epoch: 0038 loss_train: 0.4095 acc_train: 0.9333 loss_val: 0.5313 acc_val: 0.8700 time: 0.0276s\n",
            "Epoch: 0039 loss_train: 0.3814 acc_train: 0.9333 loss_val: 0.5308 acc_val: 0.8700 time: 0.0245s\n",
            "Epoch: 0040 loss_train: 0.3854 acc_train: 0.9167 loss_val: 0.5307 acc_val: 0.8700 time: 0.0247s\n",
            "Epoch: 0041 loss_train: 0.3255 acc_train: 0.9750 loss_val: 0.5315 acc_val: 0.8700 time: 0.0269s\n",
            "Epoch: 0042 loss_train: 0.4358 acc_train: 0.9417 loss_val: 0.5335 acc_val: 0.8633 time: 0.0251s\n",
            "Epoch: 0043 loss_train: 0.3792 acc_train: 0.9417 loss_val: 0.5366 acc_val: 0.8567 time: 0.0276s\n",
            "Epoch: 0044 loss_train: 0.3936 acc_train: 0.9250 loss_val: 0.5402 acc_val: 0.8567 time: 0.0261s\n",
            "Epoch: 0045 loss_train: 0.3983 acc_train: 0.9250 loss_val: 0.5424 acc_val: 0.8567 time: 0.0292s\n",
            "Epoch: 0046 loss_train: 0.3569 acc_train: 0.9500 loss_val: 0.5446 acc_val: 0.8533 time: 0.0269s\n",
            "Epoch: 0047 loss_train: 0.3879 acc_train: 0.9333 loss_val: 0.5459 acc_val: 0.8567 time: 0.0260s\n",
            "Epoch: 0048 loss_train: 0.3392 acc_train: 0.9500 loss_val: 0.5453 acc_val: 0.8567 time: 0.0249s\n",
            "Epoch: 0049 loss_train: 0.4054 acc_train: 0.9000 loss_val: 0.5444 acc_val: 0.8533 time: 0.0251s\n",
            "Epoch: 0050 loss_train: 0.3694 acc_train: 0.9500 loss_val: 0.5428 acc_val: 0.8533 time: 0.0273s\n",
            "Epoch: 0051 loss_train: 0.3417 acc_train: 0.9583 loss_val: 0.5417 acc_val: 0.8600 time: 0.0262s\n",
            "Epoch: 0052 loss_train: 0.3729 acc_train: 0.9167 loss_val: 0.5404 acc_val: 0.8667 time: 0.0281s\n",
            "Epoch: 0053 loss_train: 0.3704 acc_train: 0.9333 loss_val: 0.5393 acc_val: 0.8700 time: 0.0257s\n",
            "Epoch: 0054 loss_train: 0.3462 acc_train: 0.9333 loss_val: 0.5391 acc_val: 0.8667 time: 0.0254s\n",
            "Epoch: 0055 loss_train: 0.3478 acc_train: 0.9500 loss_val: 0.5394 acc_val: 0.8633 time: 0.0260s\n",
            "Epoch: 0056 loss_train: 0.3879 acc_train: 0.9083 loss_val: 0.5403 acc_val: 0.8600 time: 0.0253s\n",
            "Epoch: 0057 loss_train: 0.3866 acc_train: 0.9167 loss_val: 0.5408 acc_val: 0.8600 time: 0.0250s\n",
            "Epoch: 0058 loss_train: 0.3418 acc_train: 0.9500 loss_val: 0.5411 acc_val: 0.8600 time: 0.0289s\n",
            "Epoch: 0059 loss_train: 0.3461 acc_train: 0.9500 loss_val: 0.5412 acc_val: 0.8600 time: 0.0360s\n",
            "Epoch: 0060 loss_train: 0.3326 acc_train: 0.9500 loss_val: 0.5410 acc_val: 0.8600 time: 0.0253s\n",
            "Epoch: 0061 loss_train: 0.3585 acc_train: 0.9417 loss_val: 0.5403 acc_val: 0.8600 time: 0.0261s\n",
            "Epoch: 0062 loss_train: 0.3417 acc_train: 0.9333 loss_val: 0.5402 acc_val: 0.8600 time: 0.0254s\n",
            "Epoch: 0063 loss_train: 0.3519 acc_train: 0.9500 loss_val: 0.5427 acc_val: 0.8567 time: 0.0269s\n",
            "Epoch: 0064 loss_train: 0.3282 acc_train: 0.9750 loss_val: 0.5450 acc_val: 0.8567 time: 0.0250s\n",
            "Epoch: 0065 loss_train: 0.3355 acc_train: 0.9333 loss_val: 0.5468 acc_val: 0.8567 time: 0.0248s\n",
            "Epoch: 0066 loss_train: 0.3492 acc_train: 0.9583 loss_val: 0.5480 acc_val: 0.8533 time: 0.0247s\n",
            "Epoch: 0067 loss_train: 0.3330 acc_train: 0.9500 loss_val: 0.5471 acc_val: 0.8567 time: 0.0242s\n",
            "Epoch: 0068 loss_train: 0.3105 acc_train: 0.9750 loss_val: 0.5458 acc_val: 0.8600 time: 0.0245s\n",
            "Epoch: 0069 loss_train: 0.3678 acc_train: 0.9167 loss_val: 0.5449 acc_val: 0.8600 time: 0.0241s\n",
            "Epoch: 0070 loss_train: 0.3162 acc_train: 0.9250 loss_val: 0.5438 acc_val: 0.8600 time: 0.0246s\n",
            "Epoch: 0071 loss_train: 0.3734 acc_train: 0.9250 loss_val: 0.5410 acc_val: 0.8600 time: 0.0248s\n",
            "Epoch: 0072 loss_train: 0.3378 acc_train: 0.9583 loss_val: 0.5376 acc_val: 0.8633 time: 0.0240s\n",
            "Epoch: 0073 loss_train: 0.3549 acc_train: 0.9417 loss_val: 0.5356 acc_val: 0.8633 time: 0.0236s\n",
            "Epoch: 0074 loss_train: 0.3245 acc_train: 0.9750 loss_val: 0.5338 acc_val: 0.8667 time: 0.0302s\n",
            "Epoch: 0075 loss_train: 0.3020 acc_train: 0.9667 loss_val: 0.5335 acc_val: 0.8633 time: 0.0248s\n",
            "Epoch: 0076 loss_train: 0.3367 acc_train: 0.9500 loss_val: 0.5342 acc_val: 0.8633 time: 0.0247s\n",
            "Epoch: 0077 loss_train: 0.3042 acc_train: 0.9500 loss_val: 0.5346 acc_val: 0.8600 time: 0.0245s\n",
            "Epoch: 0078 loss_train: 0.3225 acc_train: 0.9750 loss_val: 0.5362 acc_val: 0.8600 time: 0.0253s\n",
            "Epoch: 0079 loss_train: 0.3600 acc_train: 0.9417 loss_val: 0.5380 acc_val: 0.8600 time: 0.0269s\n",
            "Epoch: 0080 loss_train: 0.3135 acc_train: 0.9833 loss_val: 0.5402 acc_val: 0.8633 time: 0.0252s\n",
            "Epoch: 0081 loss_train: 0.3420 acc_train: 0.9417 loss_val: 0.5422 acc_val: 0.8600 time: 0.0249s\n",
            "Epoch: 0082 loss_train: 0.3565 acc_train: 0.9250 loss_val: 0.5436 acc_val: 0.8600 time: 0.0303s\n",
            "Epoch: 0083 loss_train: 0.3204 acc_train: 0.9500 loss_val: 0.5437 acc_val: 0.8633 time: 0.0249s\n",
            "Epoch: 0084 loss_train: 0.3472 acc_train: 0.9750 loss_val: 0.5419 acc_val: 0.8600 time: 0.0252s\n",
            "Epoch: 0085 loss_train: 0.2986 acc_train: 0.9417 loss_val: 0.5401 acc_val: 0.8633 time: 0.0250s\n",
            "Epoch: 0086 loss_train: 0.2746 acc_train: 0.9833 loss_val: 0.5392 acc_val: 0.8667 time: 0.0247s\n",
            "Epoch: 0087 loss_train: 0.3193 acc_train: 0.9250 loss_val: 0.5381 acc_val: 0.8700 time: 0.0304s\n",
            "Epoch: 0088 loss_train: 0.3062 acc_train: 0.9583 loss_val: 0.5369 acc_val: 0.8667 time: 0.0258s\n",
            "Epoch: 0089 loss_train: 0.3174 acc_train: 0.9500 loss_val: 0.5356 acc_val: 0.8667 time: 0.0305s\n",
            "Epoch: 0090 loss_train: 0.3420 acc_train: 0.9583 loss_val: 0.5358 acc_val: 0.8633 time: 0.0282s\n",
            "Epoch: 0091 loss_train: 0.3238 acc_train: 0.9250 loss_val: 0.5367 acc_val: 0.8633 time: 0.0244s\n",
            "Epoch: 0092 loss_train: 0.3333 acc_train: 0.9250 loss_val: 0.5379 acc_val: 0.8600 time: 0.0321s\n",
            "Epoch: 0093 loss_train: 0.2958 acc_train: 0.9417 loss_val: 0.5385 acc_val: 0.8600 time: 0.0365s\n",
            "Epoch: 0094 loss_train: 0.2975 acc_train: 0.9750 loss_val: 0.5384 acc_val: 0.8567 time: 0.0349s\n",
            "Epoch: 0095 loss_train: 0.3267 acc_train: 0.9417 loss_val: 0.5357 acc_val: 0.8667 time: 0.0354s\n",
            "Epoch: 0096 loss_train: 0.3063 acc_train: 0.9583 loss_val: 0.5326 acc_val: 0.8700 time: 0.0339s\n",
            "Epoch: 0097 loss_train: 0.3072 acc_train: 0.9667 loss_val: 0.5303 acc_val: 0.8700 time: 0.0263s\n",
            "Epoch: 0098 loss_train: 0.2901 acc_train: 0.9583 loss_val: 0.5282 acc_val: 0.8633 time: 0.0255s\n",
            "Epoch: 0099 loss_train: 0.3177 acc_train: 0.9500 loss_val: 0.5277 acc_val: 0.8600 time: 0.0255s\n",
            "Epoch: 0100 loss_train: 0.2929 acc_train: 0.9583 loss_val: 0.5279 acc_val: 0.8633 time: 0.0253s\n",
            "Epoch: 0101 loss_train: 0.3042 acc_train: 0.9667 loss_val: 0.5297 acc_val: 0.8667 time: 0.0359s\n",
            "Epoch: 0102 loss_train: 0.2902 acc_train: 0.9750 loss_val: 0.5342 acc_val: 0.8667 time: 0.0446s\n",
            "Epoch: 0103 loss_train: 0.3030 acc_train: 0.9500 loss_val: 0.5406 acc_val: 0.8667 time: 0.0329s\n",
            "Epoch: 0104 loss_train: 0.3136 acc_train: 0.9167 loss_val: 0.5476 acc_val: 0.8633 time: 0.0270s\n",
            "Epoch: 0105 loss_train: 0.3462 acc_train: 0.9000 loss_val: 0.5473 acc_val: 0.8633 time: 0.0247s\n",
            "Epoch: 0106 loss_train: 0.3234 acc_train: 0.9333 loss_val: 0.5419 acc_val: 0.8600 time: 0.0246s\n",
            "Epoch: 0107 loss_train: 0.3077 acc_train: 0.9583 loss_val: 0.5361 acc_val: 0.8667 time: 0.0240s\n",
            "Epoch: 0108 loss_train: 0.3151 acc_train: 0.9417 loss_val: 0.5299 acc_val: 0.8700 time: 0.0279s\n",
            "Epoch: 0109 loss_train: 0.3243 acc_train: 0.9250 loss_val: 0.5264 acc_val: 0.8667 time: 0.0307s\n",
            "Epoch: 0110 loss_train: 0.3125 acc_train: 0.9583 loss_val: 0.5249 acc_val: 0.8633 time: 0.0283s\n",
            "Epoch: 0111 loss_train: 0.2866 acc_train: 0.9667 loss_val: 0.5249 acc_val: 0.8633 time: 0.0249s\n",
            "Epoch: 0112 loss_train: 0.2732 acc_train: 0.9667 loss_val: 0.5265 acc_val: 0.8633 time: 0.0257s\n",
            "Epoch: 0113 loss_train: 0.3226 acc_train: 0.9583 loss_val: 0.5290 acc_val: 0.8600 time: 0.0269s\n",
            "Epoch: 0114 loss_train: 0.2877 acc_train: 0.9833 loss_val: 0.5336 acc_val: 0.8633 time: 0.0248s\n",
            "Epoch: 0115 loss_train: 0.3305 acc_train: 0.9333 loss_val: 0.5366 acc_val: 0.8600 time: 0.0277s\n",
            "Epoch: 0116 loss_train: 0.2978 acc_train: 0.9500 loss_val: 0.5365 acc_val: 0.8600 time: 0.0320s\n",
            "Epoch: 0117 loss_train: 0.2453 acc_train: 0.9750 loss_val: 0.5347 acc_val: 0.8633 time: 0.0252s\n",
            "Epoch: 0118 loss_train: 0.2550 acc_train: 0.9667 loss_val: 0.5313 acc_val: 0.8633 time: 0.0246s\n",
            "Epoch: 0119 loss_train: 0.2833 acc_train: 0.9500 loss_val: 0.5263 acc_val: 0.8700 time: 0.0243s\n",
            "Epoch: 0120 loss_train: 0.2982 acc_train: 0.9667 loss_val: 0.5225 acc_val: 0.8667 time: 0.0245s\n",
            "Epoch: 0121 loss_train: 0.2714 acc_train: 0.9750 loss_val: 0.5215 acc_val: 0.8600 time: 0.0267s\n",
            "Epoch: 0122 loss_train: 0.2560 acc_train: 0.9583 loss_val: 0.5216 acc_val: 0.8633 time: 0.0261s\n",
            "Epoch: 0123 loss_train: 0.2868 acc_train: 0.9833 loss_val: 0.5234 acc_val: 0.8600 time: 0.0240s\n",
            "Epoch: 0124 loss_train: 0.3050 acc_train: 0.9417 loss_val: 0.5265 acc_val: 0.8667 time: 0.0293s\n",
            "Epoch: 0125 loss_train: 0.3105 acc_train: 0.9500 loss_val: 0.5310 acc_val: 0.8667 time: 0.0242s\n",
            "Epoch: 0126 loss_train: 0.2809 acc_train: 0.9750 loss_val: 0.5346 acc_val: 0.8567 time: 0.0248s\n",
            "Epoch: 0127 loss_train: 0.2547 acc_train: 0.9667 loss_val: 0.5364 acc_val: 0.8567 time: 0.0252s\n",
            "Epoch: 0128 loss_train: 0.3033 acc_train: 0.9583 loss_val: 0.5352 acc_val: 0.8567 time: 0.0282s\n",
            "Epoch: 0129 loss_train: 0.2913 acc_train: 0.9500 loss_val: 0.5318 acc_val: 0.8633 time: 0.0447s\n",
            "Epoch: 0130 loss_train: 0.2613 acc_train: 0.9333 loss_val: 0.5281 acc_val: 0.8600 time: 0.0325s\n",
            "Epoch: 0131 loss_train: 0.2663 acc_train: 0.9750 loss_val: 0.5241 acc_val: 0.8667 time: 0.0274s\n",
            "Epoch: 0132 loss_train: 0.3093 acc_train: 0.9667 loss_val: 0.5221 acc_val: 0.8667 time: 0.0236s\n",
            "Epoch: 0133 loss_train: 0.3051 acc_train: 0.9750 loss_val: 0.5224 acc_val: 0.8633 time: 0.0243s\n",
            "Epoch: 0134 loss_train: 0.3370 acc_train: 0.9333 loss_val: 0.5240 acc_val: 0.8667 time: 0.0243s\n",
            "Epoch: 0135 loss_train: 0.2961 acc_train: 0.9667 loss_val: 0.5257 acc_val: 0.8700 time: 0.0243s\n",
            "Epoch: 0136 loss_train: 0.2494 acc_train: 0.9750 loss_val: 0.5273 acc_val: 0.8667 time: 0.0240s\n",
            "Epoch: 0137 loss_train: 0.2602 acc_train: 0.9917 loss_val: 0.5282 acc_val: 0.8667 time: 0.0412s\n",
            "Epoch: 0138 loss_train: 0.2750 acc_train: 0.9500 loss_val: 0.5308 acc_val: 0.8633 time: 0.0574s\n",
            "Epoch: 0139 loss_train: 0.2515 acc_train: 0.9667 loss_val: 0.5310 acc_val: 0.8633 time: 0.0788s\n",
            "Epoch: 0140 loss_train: 0.2914 acc_train: 0.9750 loss_val: 0.5303 acc_val: 0.8633 time: 0.0737s\n",
            "Epoch: 0141 loss_train: 0.3723 acc_train: 0.9417 loss_val: 0.5315 acc_val: 0.8567 time: 0.0504s\n",
            "Epoch: 0142 loss_train: 0.2683 acc_train: 0.9750 loss_val: 0.5323 acc_val: 0.8600 time: 0.0472s\n",
            "Epoch: 0143 loss_train: 0.2691 acc_train: 0.9583 loss_val: 0.5321 acc_val: 0.8600 time: 0.0969s\n",
            "Epoch: 0144 loss_train: 0.3066 acc_train: 0.9500 loss_val: 0.5293 acc_val: 0.8667 time: 0.1071s\n",
            "Epoch: 0145 loss_train: 0.2888 acc_train: 0.9500 loss_val: 0.5257 acc_val: 0.8667 time: 0.1145s\n",
            "Epoch: 0146 loss_train: 0.2703 acc_train: 0.9750 loss_val: 0.5208 acc_val: 0.8633 time: 0.1054s\n",
            "Epoch: 0147 loss_train: 0.2753 acc_train: 0.9667 loss_val: 0.5165 acc_val: 0.8633 time: 0.0999s\n",
            "Epoch: 0148 loss_train: 0.2880 acc_train: 0.9583 loss_val: 0.5139 acc_val: 0.8667 time: 0.0896s\n",
            "Epoch: 0149 loss_train: 0.3015 acc_train: 0.9500 loss_val: 0.5140 acc_val: 0.8667 time: 0.0534s\n",
            "Epoch: 0150 loss_train: 0.2756 acc_train: 0.9750 loss_val: 0.5144 acc_val: 0.8633 time: 0.0592s\n",
            "Epoch: 0151 loss_train: 0.2663 acc_train: 0.9417 loss_val: 0.5151 acc_val: 0.8633 time: 0.0922s\n",
            "Epoch: 0152 loss_train: 0.2798 acc_train: 0.9750 loss_val: 0.5172 acc_val: 0.8600 time: 0.0880s\n",
            "Epoch: 0153 loss_train: 0.2836 acc_train: 0.9583 loss_val: 0.5193 acc_val: 0.8600 time: 0.1216s\n",
            "Epoch: 0154 loss_train: 0.2331 acc_train: 0.9833 loss_val: 0.5217 acc_val: 0.8667 time: 0.0584s\n",
            "Epoch: 0155 loss_train: 0.2941 acc_train: 0.9750 loss_val: 0.5228 acc_val: 0.8667 time: 0.0638s\n",
            "Epoch: 0156 loss_train: 0.2970 acc_train: 0.9583 loss_val: 0.5217 acc_val: 0.8633 time: 0.0541s\n",
            "Epoch: 0157 loss_train: 0.2630 acc_train: 0.9667 loss_val: 0.5194 acc_val: 0.8700 time: 0.0727s\n",
            "Epoch: 0158 loss_train: 0.2845 acc_train: 0.9417 loss_val: 0.5152 acc_val: 0.8633 time: 0.0662s\n",
            "Epoch: 0159 loss_train: 0.2408 acc_train: 0.9750 loss_val: 0.5112 acc_val: 0.8633 time: 0.0741s\n",
            "Epoch: 0160 loss_train: 0.3095 acc_train: 0.9583 loss_val: 0.5106 acc_val: 0.8600 time: 0.0922s\n",
            "Epoch: 0161 loss_train: 0.2459 acc_train: 0.9750 loss_val: 0.5116 acc_val: 0.8633 time: 0.1436s\n",
            "Epoch: 0162 loss_train: 0.2634 acc_train: 0.9750 loss_val: 0.5147 acc_val: 0.8633 time: 0.1041s\n",
            "Epoch: 0163 loss_train: 0.2433 acc_train: 0.9833 loss_val: 0.5193 acc_val: 0.8600 time: 0.1171s\n",
            "Epoch: 0164 loss_train: 0.2358 acc_train: 0.9750 loss_val: 0.5228 acc_val: 0.8600 time: 0.0736s\n",
            "Epoch: 0165 loss_train: 0.2631 acc_train: 0.9417 loss_val: 0.5254 acc_val: 0.8600 time: 0.0523s\n",
            "Epoch: 0166 loss_train: 0.2800 acc_train: 0.9667 loss_val: 0.5272 acc_val: 0.8567 time: 0.0620s\n",
            "Epoch: 0167 loss_train: 0.2820 acc_train: 0.9583 loss_val: 0.5240 acc_val: 0.8633 time: 0.0560s\n",
            "Epoch: 0168 loss_train: 0.2951 acc_train: 0.9417 loss_val: 0.5196 acc_val: 0.8633 time: 0.0523s\n",
            "Epoch: 0169 loss_train: 0.2828 acc_train: 0.9750 loss_val: 0.5160 acc_val: 0.8600 time: 0.0671s\n",
            "Epoch: 0170 loss_train: 0.2508 acc_train: 0.9583 loss_val: 0.5140 acc_val: 0.8667 time: 0.0592s\n",
            "Epoch: 0171 loss_train: 0.2640 acc_train: 0.9667 loss_val: 0.5138 acc_val: 0.8700 time: 0.0736s\n",
            "Epoch: 0172 loss_train: 0.2567 acc_train: 0.9833 loss_val: 0.5165 acc_val: 0.8667 time: 0.0699s\n",
            "Epoch: 0173 loss_train: 0.2741 acc_train: 0.9500 loss_val: 0.5199 acc_val: 0.8633 time: 0.0786s\n",
            "Epoch: 0174 loss_train: 0.3011 acc_train: 0.9333 loss_val: 0.5222 acc_val: 0.8633 time: 0.0852s\n",
            "Epoch: 0175 loss_train: 0.2777 acc_train: 0.9917 loss_val: 0.5262 acc_val: 0.8633 time: 0.0472s\n",
            "Epoch: 0176 loss_train: 0.2734 acc_train: 0.9667 loss_val: 0.5289 acc_val: 0.8567 time: 0.0478s\n",
            "Epoch: 0177 loss_train: 0.2520 acc_train: 0.9750 loss_val: 0.5291 acc_val: 0.8533 time: 0.0590s\n",
            "Epoch: 0178 loss_train: 0.2870 acc_train: 0.9333 loss_val: 0.5271 acc_val: 0.8600 time: 0.0527s\n",
            "Epoch: 0179 loss_train: 0.2765 acc_train: 0.9500 loss_val: 0.5257 acc_val: 0.8600 time: 0.0755s\n",
            "Epoch: 0180 loss_train: 0.2448 acc_train: 0.9750 loss_val: 0.5239 acc_val: 0.8600 time: 0.0523s\n",
            "Epoch: 0181 loss_train: 0.2397 acc_train: 0.9583 loss_val: 0.5208 acc_val: 0.8600 time: 0.0752s\n",
            "Epoch: 0182 loss_train: 0.2326 acc_train: 0.9917 loss_val: 0.5184 acc_val: 0.8600 time: 0.0690s\n",
            "Epoch: 0183 loss_train: 0.2496 acc_train: 0.9750 loss_val: 0.5182 acc_val: 0.8567 time: 0.0578s\n",
            "Epoch: 0184 loss_train: 0.2375 acc_train: 0.9833 loss_val: 0.5193 acc_val: 0.8600 time: 0.0453s\n",
            "Epoch: 0185 loss_train: 0.2686 acc_train: 0.9750 loss_val: 0.5204 acc_val: 0.8667 time: 0.0253s\n",
            "Epoch: 0186 loss_train: 0.2726 acc_train: 0.9833 loss_val: 0.5234 acc_val: 0.8633 time: 0.0684s\n",
            "Epoch: 0187 loss_train: 0.2993 acc_train: 0.9417 loss_val: 0.5249 acc_val: 0.8633 time: 0.0556s\n",
            "Epoch: 0188 loss_train: 0.2472 acc_train: 0.9833 loss_val: 0.5249 acc_val: 0.8633 time: 0.0357s\n",
            "Epoch: 0189 loss_train: 0.2602 acc_train: 0.9750 loss_val: 0.5244 acc_val: 0.8600 time: 0.0420s\n",
            "Epoch: 0190 loss_train: 0.2486 acc_train: 0.9583 loss_val: 0.5237 acc_val: 0.8567 time: 0.0701s\n",
            "Epoch: 0191 loss_train: 0.2457 acc_train: 0.9917 loss_val: 0.5219 acc_val: 0.8533 time: 0.0732s\n",
            "Epoch: 0192 loss_train: 0.2626 acc_train: 0.9583 loss_val: 0.5204 acc_val: 0.8533 time: 0.0821s\n",
            "Epoch: 0193 loss_train: 0.2498 acc_train: 0.9667 loss_val: 0.5211 acc_val: 0.8533 time: 0.0834s\n",
            "Epoch: 0194 loss_train: 0.2460 acc_train: 0.9833 loss_val: 0.5212 acc_val: 0.8533 time: 0.0728s\n",
            "Epoch: 0195 loss_train: 0.2630 acc_train: 0.9667 loss_val: 0.5193 acc_val: 0.8533 time: 0.0650s\n",
            "Epoch: 0196 loss_train: 0.2600 acc_train: 0.9750 loss_val: 0.5160 acc_val: 0.8533 time: 0.0456s\n",
            "Epoch: 0197 loss_train: 0.2304 acc_train: 0.9750 loss_val: 0.5129 acc_val: 0.8567 time: 0.0462s\n",
            "Epoch: 0198 loss_train: 0.2565 acc_train: 0.9500 loss_val: 0.5110 acc_val: 0.8600 time: 0.0503s\n",
            "Epoch: 0199 loss_train: 0.2170 acc_train: 0.9917 loss_val: 0.5113 acc_val: 0.8600 time: 0.0700s\n",
            "Epoch: 0200 loss_train: 0.2599 acc_train: 0.9667 loss_val: 0.5123 acc_val: 0.8600 time: 0.0535s\n",
            "Optimization Finished for 120 labeled nodes!\n",
            "Total time elapsed: 9.0111s\n",
            "Test set results: loss= 0.7389 accuracy= 0.7920\n",
            "Epoch: 0001 loss_train: 0.5662 acc_train: 0.8278 loss_val: 0.4984 acc_val: 0.8667 time: 0.0273s\n",
            "Epoch: 0002 loss_train: 0.6158 acc_train: 0.8167 loss_val: 0.4810 acc_val: 0.8767 time: 0.0447s\n",
            "Epoch: 0003 loss_train: 0.5308 acc_train: 0.8333 loss_val: 0.4657 acc_val: 0.8867 time: 0.0452s\n",
            "Epoch: 0004 loss_train: 0.5421 acc_train: 0.8889 loss_val: 0.4507 acc_val: 0.9033 time: 0.0464s\n",
            "Epoch: 0005 loss_train: 0.4846 acc_train: 0.8500 loss_val: 0.4360 acc_val: 0.9100 time: 0.0258s\n",
            "Epoch: 0006 loss_train: 0.4867 acc_train: 0.8722 loss_val: 0.4215 acc_val: 0.9333 time: 0.0286s\n",
            "Epoch: 0007 loss_train: 0.4345 acc_train: 0.9000 loss_val: 0.4085 acc_val: 0.9333 time: 0.0457s\n",
            "Epoch: 0008 loss_train: 0.4444 acc_train: 0.9000 loss_val: 0.3971 acc_val: 0.9333 time: 0.0407s\n",
            "Epoch: 0009 loss_train: 0.4629 acc_train: 0.9222 loss_val: 0.3863 acc_val: 0.9267 time: 0.0548s\n",
            "Epoch: 0010 loss_train: 0.4083 acc_train: 0.9056 loss_val: 0.3763 acc_val: 0.9267 time: 0.0256s\n",
            "Epoch: 0011 loss_train: 0.3549 acc_train: 0.9389 loss_val: 0.3670 acc_val: 0.9367 time: 0.0267s\n",
            "Epoch: 0012 loss_train: 0.3818 acc_train: 0.9111 loss_val: 0.3593 acc_val: 0.9367 time: 0.0262s\n",
            "Epoch: 0013 loss_train: 0.3990 acc_train: 0.9111 loss_val: 0.3539 acc_val: 0.9367 time: 0.0311s\n",
            "Epoch: 0014 loss_train: 0.3679 acc_train: 0.8944 loss_val: 0.3503 acc_val: 0.9367 time: 0.0269s\n",
            "Epoch: 0015 loss_train: 0.3733 acc_train: 0.9333 loss_val: 0.3476 acc_val: 0.9300 time: 0.0270s\n",
            "Epoch: 0016 loss_train: 0.3360 acc_train: 0.9333 loss_val: 0.3452 acc_val: 0.9333 time: 0.0358s\n",
            "Epoch: 0017 loss_train: 0.3867 acc_train: 0.9111 loss_val: 0.3425 acc_val: 0.9300 time: 0.0269s\n",
            "Epoch: 0018 loss_train: 0.3502 acc_train: 0.9444 loss_val: 0.3400 acc_val: 0.9233 time: 0.0265s\n",
            "Epoch: 0019 loss_train: 0.3621 acc_train: 0.9389 loss_val: 0.3370 acc_val: 0.9233 time: 0.0297s\n",
            "Epoch: 0020 loss_train: 0.3018 acc_train: 0.9667 loss_val: 0.3344 acc_val: 0.9267 time: 0.0329s\n",
            "Epoch: 0021 loss_train: 0.3037 acc_train: 0.9389 loss_val: 0.3319 acc_val: 0.9300 time: 0.0268s\n",
            "Epoch: 0022 loss_train: 0.3450 acc_train: 0.9056 loss_val: 0.3301 acc_val: 0.9300 time: 0.0264s\n",
            "Epoch: 0023 loss_train: 0.3320 acc_train: 0.9222 loss_val: 0.3290 acc_val: 0.9333 time: 0.0307s\n",
            "Epoch: 0024 loss_train: 0.3225 acc_train: 0.9500 loss_val: 0.3289 acc_val: 0.9333 time: 0.0283s\n",
            "Epoch: 0025 loss_train: 0.2901 acc_train: 0.9444 loss_val: 0.3295 acc_val: 0.9333 time: 0.0260s\n",
            "Epoch: 0026 loss_train: 0.2721 acc_train: 0.9556 loss_val: 0.3304 acc_val: 0.9333 time: 0.0290s\n",
            "Epoch: 0027 loss_train: 0.2930 acc_train: 0.9389 loss_val: 0.3309 acc_val: 0.9300 time: 0.0271s\n",
            "Epoch: 0028 loss_train: 0.3146 acc_train: 0.9333 loss_val: 0.3313 acc_val: 0.9367 time: 0.0268s\n",
            "Epoch: 0029 loss_train: 0.3012 acc_train: 0.9556 loss_val: 0.3314 acc_val: 0.9267 time: 0.0287s\n",
            "Epoch: 0030 loss_train: 0.2997 acc_train: 0.9556 loss_val: 0.3315 acc_val: 0.9233 time: 0.0358s\n",
            "Epoch: 0031 loss_train: 0.2892 acc_train: 0.9500 loss_val: 0.3320 acc_val: 0.9267 time: 0.0262s\n",
            "Epoch: 0032 loss_train: 0.2620 acc_train: 0.9611 loss_val: 0.3333 acc_val: 0.9267 time: 0.0251s\n",
            "Epoch: 0033 loss_train: 0.2771 acc_train: 0.9611 loss_val: 0.3348 acc_val: 0.9233 time: 0.0457s\n",
            "Epoch: 0034 loss_train: 0.3069 acc_train: 0.9722 loss_val: 0.3363 acc_val: 0.9200 time: 0.0420s\n",
            "Epoch: 0035 loss_train: 0.3150 acc_train: 0.9444 loss_val: 0.3373 acc_val: 0.9200 time: 0.0262s\n",
            "Epoch: 0036 loss_train: 0.3061 acc_train: 0.9556 loss_val: 0.3383 acc_val: 0.9233 time: 0.0460s\n",
            "Epoch: 0037 loss_train: 0.3165 acc_train: 0.9444 loss_val: 0.3395 acc_val: 0.9233 time: 0.0259s\n",
            "Epoch: 0038 loss_train: 0.2867 acc_train: 0.9500 loss_val: 0.3410 acc_val: 0.9233 time: 0.0264s\n",
            "Epoch: 0039 loss_train: 0.2927 acc_train: 0.9500 loss_val: 0.3427 acc_val: 0.9233 time: 0.0261s\n",
            "Epoch: 0040 loss_train: 0.2670 acc_train: 0.9611 loss_val: 0.3438 acc_val: 0.9200 time: 0.0263s\n",
            "Epoch: 0041 loss_train: 0.2994 acc_train: 0.9389 loss_val: 0.3439 acc_val: 0.9200 time: 0.0266s\n",
            "Epoch: 0042 loss_train: 0.2939 acc_train: 0.9556 loss_val: 0.3438 acc_val: 0.9167 time: 0.0262s\n",
            "Epoch: 0043 loss_train: 0.3113 acc_train: 0.9500 loss_val: 0.3438 acc_val: 0.9200 time: 0.0333s\n",
            "Epoch: 0044 loss_train: 0.2787 acc_train: 0.9667 loss_val: 0.3446 acc_val: 0.9200 time: 0.0261s\n",
            "Epoch: 0045 loss_train: 0.2522 acc_train: 0.9556 loss_val: 0.3456 acc_val: 0.9167 time: 0.0398s\n",
            "Epoch: 0046 loss_train: 0.2646 acc_train: 0.9667 loss_val: 0.3466 acc_val: 0.9167 time: 0.0263s\n",
            "Epoch: 0047 loss_train: 0.3071 acc_train: 0.9500 loss_val: 0.3473 acc_val: 0.9167 time: 0.0342s\n",
            "Epoch: 0048 loss_train: 0.3013 acc_train: 0.9444 loss_val: 0.3480 acc_val: 0.9200 time: 0.0269s\n",
            "Epoch: 0049 loss_train: 0.3076 acc_train: 0.9444 loss_val: 0.3483 acc_val: 0.9200 time: 0.0388s\n",
            "Epoch: 0050 loss_train: 0.2800 acc_train: 0.9667 loss_val: 0.3483 acc_val: 0.9200 time: 0.0262s\n",
            "Epoch: 0051 loss_train: 0.2888 acc_train: 0.9722 loss_val: 0.3482 acc_val: 0.9200 time: 0.0253s\n",
            "Epoch: 0052 loss_train: 0.3117 acc_train: 0.9556 loss_val: 0.3488 acc_val: 0.9200 time: 0.0430s\n",
            "Epoch: 0053 loss_train: 0.3263 acc_train: 0.9333 loss_val: 0.3494 acc_val: 0.9167 time: 0.0313s\n",
            "Epoch: 0054 loss_train: 0.3249 acc_train: 0.9444 loss_val: 0.3499 acc_val: 0.9133 time: 0.0281s\n",
            "Epoch: 0055 loss_train: 0.3098 acc_train: 0.9389 loss_val: 0.3503 acc_val: 0.9167 time: 0.0331s\n",
            "Epoch: 0056 loss_train: 0.2935 acc_train: 0.9667 loss_val: 0.3505 acc_val: 0.9167 time: 0.0257s\n",
            "Epoch: 0057 loss_train: 0.2709 acc_train: 0.9611 loss_val: 0.3505 acc_val: 0.9200 time: 0.0280s\n",
            "Epoch: 0058 loss_train: 0.2824 acc_train: 0.9556 loss_val: 0.3507 acc_val: 0.9200 time: 0.0276s\n",
            "Epoch: 0059 loss_train: 0.2808 acc_train: 0.9722 loss_val: 0.3509 acc_val: 0.9200 time: 0.0258s\n",
            "Epoch: 0060 loss_train: 0.2705 acc_train: 0.9722 loss_val: 0.3513 acc_val: 0.9200 time: 0.0257s\n",
            "Epoch: 0061 loss_train: 0.2968 acc_train: 0.9556 loss_val: 0.3517 acc_val: 0.9200 time: 0.0253s\n",
            "Epoch: 0062 loss_train: 0.2761 acc_train: 0.9556 loss_val: 0.3522 acc_val: 0.9200 time: 0.0406s\n",
            "Epoch: 0063 loss_train: 0.2917 acc_train: 0.9389 loss_val: 0.3525 acc_val: 0.9200 time: 0.0502s\n",
            "Epoch: 0064 loss_train: 0.3072 acc_train: 0.9500 loss_val: 0.3525 acc_val: 0.9133 time: 0.0374s\n",
            "Epoch: 0065 loss_train: 0.2727 acc_train: 0.9500 loss_val: 0.3526 acc_val: 0.9133 time: 0.0757s\n",
            "Epoch: 0066 loss_train: 0.2784 acc_train: 0.9500 loss_val: 0.3527 acc_val: 0.9133 time: 0.0552s\n",
            "Epoch: 0067 loss_train: 0.2717 acc_train: 0.9611 loss_val: 0.3525 acc_val: 0.9133 time: 0.0255s\n",
            "Epoch: 0068 loss_train: 0.3058 acc_train: 0.9389 loss_val: 0.3525 acc_val: 0.9133 time: 0.0262s\n",
            "Epoch: 0069 loss_train: 0.2968 acc_train: 0.9556 loss_val: 0.3525 acc_val: 0.9167 time: 0.0753s\n",
            "Epoch: 0070 loss_train: 0.3151 acc_train: 0.9389 loss_val: 0.3529 acc_val: 0.9200 time: 0.0452s\n",
            "Epoch: 0071 loss_train: 0.2814 acc_train: 0.9333 loss_val: 0.3531 acc_val: 0.9233 time: 0.0275s\n",
            "Epoch: 0072 loss_train: 0.2970 acc_train: 0.9778 loss_val: 0.3534 acc_val: 0.9233 time: 0.0187s\n",
            "Epoch: 0073 loss_train: 0.2732 acc_train: 0.9722 loss_val: 0.3539 acc_val: 0.9200 time: 0.0191s\n",
            "Epoch: 0074 loss_train: 0.2868 acc_train: 0.9611 loss_val: 0.3547 acc_val: 0.9200 time: 0.0192s\n",
            "Epoch: 0075 loss_train: 0.3181 acc_train: 0.9500 loss_val: 0.3556 acc_val: 0.9133 time: 0.0211s\n",
            "Epoch: 0076 loss_train: 0.2931 acc_train: 0.9500 loss_val: 0.3565 acc_val: 0.9133 time: 0.0175s\n",
            "Epoch: 0077 loss_train: 0.2466 acc_train: 0.9611 loss_val: 0.3567 acc_val: 0.9133 time: 0.0171s\n",
            "Epoch: 0078 loss_train: 0.2958 acc_train: 0.9500 loss_val: 0.3561 acc_val: 0.9133 time: 0.0171s\n",
            "Epoch: 0079 loss_train: 0.2865 acc_train: 0.9500 loss_val: 0.3546 acc_val: 0.9167 time: 0.0173s\n",
            "Epoch: 0080 loss_train: 0.2896 acc_train: 0.9722 loss_val: 0.3532 acc_val: 0.9167 time: 0.0244s\n",
            "Epoch: 0081 loss_train: 0.2935 acc_train: 0.9500 loss_val: 0.3525 acc_val: 0.9167 time: 0.0224s\n",
            "Epoch: 0082 loss_train: 0.2946 acc_train: 0.9611 loss_val: 0.3518 acc_val: 0.9200 time: 0.0184s\n",
            "Epoch: 0083 loss_train: 0.3057 acc_train: 0.9444 loss_val: 0.3511 acc_val: 0.9233 time: 0.0193s\n",
            "Epoch: 0084 loss_train: 0.2684 acc_train: 0.9500 loss_val: 0.3508 acc_val: 0.9233 time: 0.0180s\n",
            "Epoch: 0085 loss_train: 0.2985 acc_train: 0.9500 loss_val: 0.3507 acc_val: 0.9200 time: 0.0178s\n",
            "Epoch: 0086 loss_train: 0.2817 acc_train: 0.9556 loss_val: 0.3511 acc_val: 0.9200 time: 0.0180s\n",
            "Epoch: 0087 loss_train: 0.2801 acc_train: 0.9556 loss_val: 0.3521 acc_val: 0.9200 time: 0.0188s\n",
            "Epoch: 0088 loss_train: 0.2634 acc_train: 0.9556 loss_val: 0.3532 acc_val: 0.9200 time: 0.0176s\n",
            "Epoch: 0089 loss_train: 0.2455 acc_train: 0.9611 loss_val: 0.3541 acc_val: 0.9167 time: 0.0196s\n",
            "Epoch: 0090 loss_train: 0.2771 acc_train: 0.9556 loss_val: 0.3546 acc_val: 0.9167 time: 0.0238s\n",
            "Epoch: 0091 loss_train: 0.2642 acc_train: 0.9556 loss_val: 0.3549 acc_val: 0.9167 time: 0.0174s\n",
            "Epoch: 0092 loss_train: 0.2488 acc_train: 0.9500 loss_val: 0.3545 acc_val: 0.9167 time: 0.0176s\n",
            "Epoch: 0093 loss_train: 0.3083 acc_train: 0.9333 loss_val: 0.3539 acc_val: 0.9167 time: 0.0182s\n",
            "Epoch: 0094 loss_train: 0.3095 acc_train: 0.9500 loss_val: 0.3533 acc_val: 0.9167 time: 0.0176s\n",
            "Epoch: 0095 loss_train: 0.2847 acc_train: 0.9611 loss_val: 0.3522 acc_val: 0.9200 time: 0.0189s\n",
            "Epoch: 0096 loss_train: 0.2667 acc_train: 0.9833 loss_val: 0.3516 acc_val: 0.9200 time: 0.0173s\n",
            "Epoch: 0097 loss_train: 0.2981 acc_train: 0.9500 loss_val: 0.3511 acc_val: 0.9233 time: 0.0193s\n",
            "Epoch: 0098 loss_train: 0.2764 acc_train: 0.9389 loss_val: 0.3512 acc_val: 0.9200 time: 0.0320s\n",
            "Epoch: 0099 loss_train: 0.2977 acc_train: 0.9500 loss_val: 0.3517 acc_val: 0.9167 time: 0.0178s\n",
            "Epoch: 0100 loss_train: 0.2481 acc_train: 0.9722 loss_val: 0.3523 acc_val: 0.9167 time: 0.0223s\n",
            "Epoch: 0101 loss_train: 0.2646 acc_train: 0.9778 loss_val: 0.3530 acc_val: 0.9133 time: 0.0171s\n",
            "Epoch: 0102 loss_train: 0.2575 acc_train: 0.9611 loss_val: 0.3539 acc_val: 0.9133 time: 0.0179s\n",
            "Epoch: 0103 loss_train: 0.2546 acc_train: 0.9722 loss_val: 0.3546 acc_val: 0.9067 time: 0.0173s\n",
            "Epoch: 0104 loss_train: 0.2837 acc_train: 0.9556 loss_val: 0.3557 acc_val: 0.9067 time: 0.0172s\n",
            "Epoch: 0105 loss_train: 0.2888 acc_train: 0.9556 loss_val: 0.3565 acc_val: 0.9033 time: 0.0200s\n",
            "Epoch: 0106 loss_train: 0.2798 acc_train: 0.9500 loss_val: 0.3565 acc_val: 0.9033 time: 0.0173s\n",
            "Epoch: 0107 loss_train: 0.2771 acc_train: 0.9556 loss_val: 0.3558 acc_val: 0.9067 time: 0.0181s\n",
            "Epoch: 0108 loss_train: 0.2821 acc_train: 0.9500 loss_val: 0.3559 acc_val: 0.9067 time: 0.0184s\n",
            "Epoch: 0109 loss_train: 0.2832 acc_train: 0.9444 loss_val: 0.3567 acc_val: 0.9100 time: 0.0176s\n",
            "Epoch: 0110 loss_train: 0.2625 acc_train: 0.9556 loss_val: 0.3569 acc_val: 0.9100 time: 0.0218s\n",
            "Epoch: 0111 loss_train: 0.2675 acc_train: 0.9500 loss_val: 0.3556 acc_val: 0.9133 time: 0.0203s\n",
            "Epoch: 0112 loss_train: 0.2547 acc_train: 0.9722 loss_val: 0.3531 acc_val: 0.9200 time: 0.0181s\n",
            "Epoch: 0113 loss_train: 0.2536 acc_train: 0.9667 loss_val: 0.3511 acc_val: 0.9200 time: 0.0185s\n",
            "Epoch: 0114 loss_train: 0.2615 acc_train: 0.9500 loss_val: 0.3497 acc_val: 0.9200 time: 0.0182s\n",
            "Epoch: 0115 loss_train: 0.2488 acc_train: 0.9500 loss_val: 0.3491 acc_val: 0.9133 time: 0.0172s\n",
            "Epoch: 0116 loss_train: 0.3062 acc_train: 0.9444 loss_val: 0.3487 acc_val: 0.9133 time: 0.0191s\n",
            "Epoch: 0117 loss_train: 0.2638 acc_train: 0.9556 loss_val: 0.3485 acc_val: 0.9200 time: 0.0171s\n",
            "Epoch: 0118 loss_train: 0.3132 acc_train: 0.9444 loss_val: 0.3488 acc_val: 0.9200 time: 0.0170s\n",
            "Epoch: 0119 loss_train: 0.2791 acc_train: 0.9722 loss_val: 0.3501 acc_val: 0.9200 time: 0.0177s\n",
            "Epoch: 0120 loss_train: 0.2834 acc_train: 0.9500 loss_val: 0.3515 acc_val: 0.9200 time: 0.0201s\n",
            "Epoch: 0121 loss_train: 0.2632 acc_train: 0.9556 loss_val: 0.3533 acc_val: 0.9167 time: 0.0192s\n",
            "Epoch: 0122 loss_train: 0.2539 acc_train: 0.9722 loss_val: 0.3553 acc_val: 0.9133 time: 0.0178s\n",
            "Epoch: 0123 loss_train: 0.2624 acc_train: 0.9611 loss_val: 0.3565 acc_val: 0.9133 time: 0.0185s\n",
            "Epoch: 0124 loss_train: 0.2699 acc_train: 0.9556 loss_val: 0.3571 acc_val: 0.9033 time: 0.0171s\n",
            "Epoch: 0125 loss_train: 0.3054 acc_train: 0.9500 loss_val: 0.3570 acc_val: 0.9033 time: 0.0170s\n",
            "Epoch: 0126 loss_train: 0.2895 acc_train: 0.9333 loss_val: 0.3561 acc_val: 0.9067 time: 0.0172s\n",
            "Epoch: 0127 loss_train: 0.2604 acc_train: 0.9444 loss_val: 0.3548 acc_val: 0.9100 time: 0.0200s\n",
            "Epoch: 0128 loss_train: 0.2686 acc_train: 0.9500 loss_val: 0.3535 acc_val: 0.9100 time: 0.0169s\n",
            "Epoch: 0129 loss_train: 0.2934 acc_train: 0.9444 loss_val: 0.3524 acc_val: 0.9133 time: 0.0185s\n",
            "Epoch: 0130 loss_train: 0.2686 acc_train: 0.9722 loss_val: 0.3518 acc_val: 0.9133 time: 0.0193s\n",
            "Epoch: 0131 loss_train: 0.2845 acc_train: 0.9611 loss_val: 0.3516 acc_val: 0.9133 time: 0.0228s\n",
            "Epoch: 0132 loss_train: 0.2515 acc_train: 0.9722 loss_val: 0.3516 acc_val: 0.9133 time: 0.0183s\n",
            "Epoch: 0133 loss_train: 0.2992 acc_train: 0.9500 loss_val: 0.3515 acc_val: 0.9200 time: 0.0194s\n",
            "Epoch: 0134 loss_train: 0.2589 acc_train: 0.9667 loss_val: 0.3514 acc_val: 0.9133 time: 0.0182s\n",
            "Epoch: 0135 loss_train: 0.2579 acc_train: 0.9722 loss_val: 0.3515 acc_val: 0.9133 time: 0.0184s\n",
            "Epoch: 0136 loss_train: 0.2524 acc_train: 0.9667 loss_val: 0.3517 acc_val: 0.9133 time: 0.0179s\n",
            "Epoch: 0137 loss_train: 0.2706 acc_train: 0.9611 loss_val: 0.3521 acc_val: 0.9100 time: 0.0181s\n",
            "Epoch: 0138 loss_train: 0.2979 acc_train: 0.9556 loss_val: 0.3523 acc_val: 0.9100 time: 0.0208s\n",
            "Epoch: 0139 loss_train: 0.2345 acc_train: 0.9833 loss_val: 0.3531 acc_val: 0.9100 time: 0.0181s\n",
            "Epoch: 0140 loss_train: 0.2856 acc_train: 0.9667 loss_val: 0.3542 acc_val: 0.9067 time: 0.0212s\n",
            "Epoch: 0141 loss_train: 0.2475 acc_train: 0.9556 loss_val: 0.3550 acc_val: 0.9067 time: 0.0177s\n",
            "Epoch: 0142 loss_train: 0.2799 acc_train: 0.9500 loss_val: 0.3551 acc_val: 0.9067 time: 0.0195s\n",
            "Epoch: 0143 loss_train: 0.2669 acc_train: 0.9889 loss_val: 0.3549 acc_val: 0.9067 time: 0.0187s\n",
            "Epoch: 0144 loss_train: 0.2677 acc_train: 0.9556 loss_val: 0.3537 acc_val: 0.9067 time: 0.0184s\n",
            "Epoch: 0145 loss_train: 0.2691 acc_train: 0.9667 loss_val: 0.3525 acc_val: 0.9100 time: 0.0221s\n",
            "Epoch: 0146 loss_train: 0.2654 acc_train: 0.9722 loss_val: 0.3518 acc_val: 0.9067 time: 0.0265s\n",
            "Epoch: 0147 loss_train: 0.2491 acc_train: 0.9611 loss_val: 0.3515 acc_val: 0.9100 time: 0.0184s\n",
            "Epoch: 0148 loss_train: 0.2853 acc_train: 0.9444 loss_val: 0.3519 acc_val: 0.9200 time: 0.0177s\n",
            "Epoch: 0149 loss_train: 0.2677 acc_train: 0.9556 loss_val: 0.3521 acc_val: 0.9233 time: 0.0176s\n",
            "Epoch: 0150 loss_train: 0.2712 acc_train: 0.9444 loss_val: 0.3511 acc_val: 0.9267 time: 0.0241s\n",
            "Epoch: 0151 loss_train: 0.2644 acc_train: 0.9611 loss_val: 0.3501 acc_val: 0.9267 time: 0.0165s\n",
            "Epoch: 0152 loss_train: 0.2542 acc_train: 0.9889 loss_val: 0.3493 acc_val: 0.9167 time: 0.0188s\n",
            "Epoch: 0153 loss_train: 0.2626 acc_train: 0.9667 loss_val: 0.3489 acc_val: 0.9133 time: 0.0173s\n",
            "Epoch: 0154 loss_train: 0.2361 acc_train: 0.9778 loss_val: 0.3491 acc_val: 0.9133 time: 0.0182s\n",
            "Epoch: 0155 loss_train: 0.2828 acc_train: 0.9389 loss_val: 0.3495 acc_val: 0.9100 time: 0.0176s\n",
            "Epoch: 0156 loss_train: 0.2987 acc_train: 0.9389 loss_val: 0.3501 acc_val: 0.9100 time: 0.0204s\n",
            "Epoch: 0157 loss_train: 0.2935 acc_train: 0.9444 loss_val: 0.3505 acc_val: 0.9100 time: 0.0179s\n",
            "Epoch: 0158 loss_train: 0.2538 acc_train: 0.9556 loss_val: 0.3512 acc_val: 0.9100 time: 0.0174s\n",
            "Epoch: 0159 loss_train: 0.2874 acc_train: 0.9444 loss_val: 0.3518 acc_val: 0.9100 time: 0.0185s\n",
            "Epoch: 0160 loss_train: 0.3032 acc_train: 0.9500 loss_val: 0.3522 acc_val: 0.9100 time: 0.0211s\n",
            "Epoch: 0161 loss_train: 0.2875 acc_train: 0.9611 loss_val: 0.3528 acc_val: 0.9100 time: 0.0174s\n",
            "Epoch: 0162 loss_train: 0.2562 acc_train: 0.9611 loss_val: 0.3522 acc_val: 0.9100 time: 0.0171s\n",
            "Epoch: 0163 loss_train: 0.2603 acc_train: 0.9778 loss_val: 0.3510 acc_val: 0.9133 time: 0.0175s\n",
            "Epoch: 0164 loss_train: 0.2455 acc_train: 0.9667 loss_val: 0.3492 acc_val: 0.9167 time: 0.0176s\n",
            "Epoch: 0165 loss_train: 0.2532 acc_train: 0.9778 loss_val: 0.3484 acc_val: 0.9233 time: 0.0174s\n",
            "Epoch: 0166 loss_train: 0.2367 acc_train: 0.9722 loss_val: 0.3484 acc_val: 0.9200 time: 0.0177s\n",
            "Epoch: 0167 loss_train: 0.2502 acc_train: 0.9722 loss_val: 0.3479 acc_val: 0.9200 time: 0.0174s\n",
            "Epoch: 0168 loss_train: 0.2478 acc_train: 0.9722 loss_val: 0.3475 acc_val: 0.9200 time: 0.0206s\n",
            "Epoch: 0169 loss_train: 0.2408 acc_train: 0.9556 loss_val: 0.3468 acc_val: 0.9200 time: 0.0223s\n",
            "Epoch: 0170 loss_train: 0.2684 acc_train: 0.9778 loss_val: 0.3463 acc_val: 0.9133 time: 0.0177s\n",
            "Epoch: 0171 loss_train: 0.2860 acc_train: 0.9556 loss_val: 0.3467 acc_val: 0.9100 time: 0.0188s\n",
            "Epoch: 0172 loss_train: 0.2375 acc_train: 0.9667 loss_val: 0.3476 acc_val: 0.9067 time: 0.0179s\n",
            "Epoch: 0173 loss_train: 0.2522 acc_train: 0.9722 loss_val: 0.3486 acc_val: 0.9100 time: 0.0211s\n",
            "Epoch: 0174 loss_train: 0.2382 acc_train: 0.9500 loss_val: 0.3493 acc_val: 0.9100 time: 0.0181s\n",
            "Epoch: 0175 loss_train: 0.2703 acc_train: 0.9333 loss_val: 0.3496 acc_val: 0.9067 time: 0.0180s\n",
            "Epoch: 0176 loss_train: 0.2635 acc_train: 0.9667 loss_val: 0.3501 acc_val: 0.9067 time: 0.0177s\n",
            "Epoch: 0177 loss_train: 0.2372 acc_train: 0.9667 loss_val: 0.3504 acc_val: 0.9067 time: 0.0179s\n",
            "Epoch: 0178 loss_train: 0.3134 acc_train: 0.9278 loss_val: 0.3500 acc_val: 0.9067 time: 0.0181s\n",
            "Epoch: 0179 loss_train: 0.2230 acc_train: 0.9722 loss_val: 0.3493 acc_val: 0.9067 time: 0.0206s\n",
            "Epoch: 0180 loss_train: 0.2830 acc_train: 0.9500 loss_val: 0.3491 acc_val: 0.9133 time: 0.0194s\n",
            "Epoch: 0181 loss_train: 0.2824 acc_train: 0.9667 loss_val: 0.3491 acc_val: 0.9100 time: 0.0179s\n",
            "Epoch: 0182 loss_train: 0.2540 acc_train: 0.9778 loss_val: 0.3495 acc_val: 0.9100 time: 0.0181s\n",
            "Epoch: 0183 loss_train: 0.2364 acc_train: 0.9667 loss_val: 0.3489 acc_val: 0.9133 time: 0.0184s\n",
            "Epoch: 0184 loss_train: 0.2598 acc_train: 0.9667 loss_val: 0.3483 acc_val: 0.9133 time: 0.0184s\n",
            "Epoch: 0185 loss_train: 0.2638 acc_train: 0.9500 loss_val: 0.3479 acc_val: 0.9100 time: 0.0208s\n",
            "Epoch: 0186 loss_train: 0.2386 acc_train: 0.9833 loss_val: 0.3479 acc_val: 0.9100 time: 0.0225s\n",
            "Epoch: 0187 loss_train: 0.2688 acc_train: 0.9667 loss_val: 0.3480 acc_val: 0.9100 time: 0.0185s\n",
            "Epoch: 0188 loss_train: 0.2749 acc_train: 0.9389 loss_val: 0.3488 acc_val: 0.9100 time: 0.0189s\n",
            "Epoch: 0189 loss_train: 0.2486 acc_train: 0.9833 loss_val: 0.3497 acc_val: 0.9100 time: 0.0232s\n",
            "Epoch: 0190 loss_train: 0.2661 acc_train: 0.9611 loss_val: 0.3499 acc_val: 0.9100 time: 0.0192s\n",
            "Epoch: 0191 loss_train: 0.2583 acc_train: 0.9778 loss_val: 0.3494 acc_val: 0.9133 time: 0.0197s\n",
            "Epoch: 0192 loss_train: 0.2770 acc_train: 0.9611 loss_val: 0.3486 acc_val: 0.9133 time: 0.0189s\n",
            "Epoch: 0193 loss_train: 0.2744 acc_train: 0.9556 loss_val: 0.3482 acc_val: 0.9133 time: 0.0251s\n",
            "Epoch: 0194 loss_train: 0.2563 acc_train: 0.9722 loss_val: 0.3481 acc_val: 0.9133 time: 0.0184s\n",
            "Epoch: 0195 loss_train: 0.2337 acc_train: 0.9778 loss_val: 0.3481 acc_val: 0.9100 time: 0.0181s\n",
            "Epoch: 0196 loss_train: 0.2600 acc_train: 0.9500 loss_val: 0.3481 acc_val: 0.9100 time: 0.0183s\n",
            "Epoch: 0197 loss_train: 0.2629 acc_train: 0.9722 loss_val: 0.3484 acc_val: 0.9100 time: 0.0180s\n",
            "Epoch: 0198 loss_train: 0.2701 acc_train: 0.9389 loss_val: 0.3486 acc_val: 0.9100 time: 0.0177s\n",
            "Epoch: 0199 loss_train: 0.3038 acc_train: 0.9389 loss_val: 0.3489 acc_val: 0.9100 time: 0.0222s\n",
            "Epoch: 0200 loss_train: 0.2471 acc_train: 0.9722 loss_val: 0.3494 acc_val: 0.9133 time: 0.0200s\n",
            "Optimization Finished for 180 labeled nodes!\n",
            "Total time elapsed: 5.4652s\n",
            "Test set results: loss= 0.6681 accuracy= 0.7890\n",
            "Epoch: 0001 loss_train: 0.3902 acc_train: 0.8917 loss_val: 0.3462 acc_val: 0.9100 time: 0.0184s\n",
            "Epoch: 0002 loss_train: 0.4289 acc_train: 0.8792 loss_val: 0.3401 acc_val: 0.9233 time: 0.0197s\n",
            "Epoch: 0003 loss_train: 0.4299 acc_train: 0.8833 loss_val: 0.3329 acc_val: 0.9267 time: 0.0191s\n",
            "Epoch: 0004 loss_train: 0.4121 acc_train: 0.8833 loss_val: 0.3255 acc_val: 0.9300 time: 0.0186s\n",
            "Epoch: 0005 loss_train: 0.3846 acc_train: 0.9167 loss_val: 0.3187 acc_val: 0.9267 time: 0.0178s\n",
            "Epoch: 0006 loss_train: 0.3942 acc_train: 0.9042 loss_val: 0.3134 acc_val: 0.9400 time: 0.0183s\n",
            "Epoch: 0007 loss_train: 0.3399 acc_train: 0.9375 loss_val: 0.3080 acc_val: 0.9433 time: 0.0180s\n",
            "Epoch: 0008 loss_train: 0.3707 acc_train: 0.9250 loss_val: 0.3023 acc_val: 0.9467 time: 0.0206s\n",
            "Epoch: 0009 loss_train: 0.3724 acc_train: 0.9333 loss_val: 0.2958 acc_val: 0.9500 time: 0.0170s\n",
            "Epoch: 0010 loss_train: 0.3631 acc_train: 0.9083 loss_val: 0.2888 acc_val: 0.9467 time: 0.0193s\n",
            "Epoch: 0011 loss_train: 0.3655 acc_train: 0.9417 loss_val: 0.2816 acc_val: 0.9533 time: 0.0173s\n",
            "Epoch: 0012 loss_train: 0.3315 acc_train: 0.9208 loss_val: 0.2761 acc_val: 0.9567 time: 0.0176s\n",
            "Epoch: 0013 loss_train: 0.3271 acc_train: 0.9417 loss_val: 0.2710 acc_val: 0.9600 time: 0.0178s\n",
            "Epoch: 0014 loss_train: 0.3052 acc_train: 0.9458 loss_val: 0.2669 acc_val: 0.9600 time: 0.0207s\n",
            "Epoch: 0015 loss_train: 0.3262 acc_train: 0.9375 loss_val: 0.2636 acc_val: 0.9600 time: 0.0174s\n",
            "Epoch: 0016 loss_train: 0.3379 acc_train: 0.9292 loss_val: 0.2609 acc_val: 0.9600 time: 0.0178s\n",
            "Epoch: 0017 loss_train: 0.3165 acc_train: 0.9333 loss_val: 0.2592 acc_val: 0.9600 time: 0.0175s\n",
            "Epoch: 0018 loss_train: 0.3157 acc_train: 0.9417 loss_val: 0.2579 acc_val: 0.9600 time: 0.0195s\n",
            "Epoch: 0019 loss_train: 0.3168 acc_train: 0.9500 loss_val: 0.2568 acc_val: 0.9667 time: 0.0195s\n",
            "Epoch: 0020 loss_train: 0.2850 acc_train: 0.9458 loss_val: 0.2557 acc_val: 0.9600 time: 0.0178s\n",
            "Epoch: 0021 loss_train: 0.2882 acc_train: 0.9542 loss_val: 0.2545 acc_val: 0.9600 time: 0.0186s\n",
            "Epoch: 0022 loss_train: 0.3437 acc_train: 0.9208 loss_val: 0.2538 acc_val: 0.9600 time: 0.0190s\n",
            "Epoch: 0023 loss_train: 0.2804 acc_train: 0.9583 loss_val: 0.2542 acc_val: 0.9667 time: 0.0176s\n",
            "Epoch: 0024 loss_train: 0.3000 acc_train: 0.9458 loss_val: 0.2556 acc_val: 0.9633 time: 0.0261s\n",
            "Epoch: 0025 loss_train: 0.3259 acc_train: 0.9250 loss_val: 0.2571 acc_val: 0.9633 time: 0.0179s\n",
            "Epoch: 0026 loss_train: 0.2986 acc_train: 0.9417 loss_val: 0.2574 acc_val: 0.9633 time: 0.0189s\n",
            "Epoch: 0027 loss_train: 0.2909 acc_train: 0.9542 loss_val: 0.2571 acc_val: 0.9667 time: 0.0182s\n",
            "Epoch: 0028 loss_train: 0.3173 acc_train: 0.9375 loss_val: 0.2563 acc_val: 0.9667 time: 0.0227s\n",
            "Epoch: 0029 loss_train: 0.2933 acc_train: 0.9583 loss_val: 0.2555 acc_val: 0.9667 time: 0.0176s\n",
            "Epoch: 0030 loss_train: 0.2823 acc_train: 0.9625 loss_val: 0.2551 acc_val: 0.9667 time: 0.0176s\n",
            "Epoch: 0031 loss_train: 0.3078 acc_train: 0.9417 loss_val: 0.2549 acc_val: 0.9633 time: 0.0177s\n",
            "Epoch: 0032 loss_train: 0.3034 acc_train: 0.9542 loss_val: 0.2547 acc_val: 0.9633 time: 0.0172s\n",
            "Epoch: 0033 loss_train: 0.2709 acc_train: 0.9583 loss_val: 0.2543 acc_val: 0.9633 time: 0.0170s\n",
            "Epoch: 0034 loss_train: 0.2836 acc_train: 0.9542 loss_val: 0.2538 acc_val: 0.9600 time: 0.0168s\n",
            "Epoch: 0035 loss_train: 0.3080 acc_train: 0.9333 loss_val: 0.2537 acc_val: 0.9600 time: 0.0175s\n",
            "Epoch: 0036 loss_train: 0.2858 acc_train: 0.9583 loss_val: 0.2536 acc_val: 0.9600 time: 0.0195s\n",
            "Epoch: 0037 loss_train: 0.2982 acc_train: 0.9458 loss_val: 0.2538 acc_val: 0.9633 time: 0.0213s\n",
            "Epoch: 0038 loss_train: 0.3039 acc_train: 0.9625 loss_val: 0.2536 acc_val: 0.9633 time: 0.0206s\n",
            "Epoch: 0039 loss_train: 0.2911 acc_train: 0.9542 loss_val: 0.2536 acc_val: 0.9633 time: 0.0178s\n",
            "Epoch: 0040 loss_train: 0.2880 acc_train: 0.9500 loss_val: 0.2539 acc_val: 0.9633 time: 0.0227s\n",
            "Epoch: 0041 loss_train: 0.2990 acc_train: 0.9375 loss_val: 0.2544 acc_val: 0.9633 time: 0.0179s\n",
            "Epoch: 0042 loss_train: 0.2823 acc_train: 0.9542 loss_val: 0.2543 acc_val: 0.9667 time: 0.0169s\n",
            "Epoch: 0043 loss_train: 0.3102 acc_train: 0.9458 loss_val: 0.2541 acc_val: 0.9633 time: 0.0207s\n",
            "Epoch: 0044 loss_train: 0.2755 acc_train: 0.9417 loss_val: 0.2542 acc_val: 0.9633 time: 0.0177s\n",
            "Epoch: 0045 loss_train: 0.3213 acc_train: 0.9500 loss_val: 0.2547 acc_val: 0.9633 time: 0.0183s\n",
            "Epoch: 0046 loss_train: 0.2942 acc_train: 0.9708 loss_val: 0.2555 acc_val: 0.9633 time: 0.0178s\n",
            "Epoch: 0047 loss_train: 0.2877 acc_train: 0.9417 loss_val: 0.2562 acc_val: 0.9633 time: 0.0190s\n",
            "Epoch: 0048 loss_train: 0.2788 acc_train: 0.9708 loss_val: 0.2567 acc_val: 0.9600 time: 0.0208s\n",
            "Epoch: 0049 loss_train: 0.3087 acc_train: 0.9417 loss_val: 0.2566 acc_val: 0.9600 time: 0.0188s\n",
            "Epoch: 0050 loss_train: 0.3006 acc_train: 0.9667 loss_val: 0.2564 acc_val: 0.9633 time: 0.0177s\n",
            "Epoch: 0051 loss_train: 0.3030 acc_train: 0.9583 loss_val: 0.2561 acc_val: 0.9633 time: 0.0179s\n",
            "Epoch: 0052 loss_train: 0.2872 acc_train: 0.9583 loss_val: 0.2555 acc_val: 0.9633 time: 0.0184s\n",
            "Epoch: 0053 loss_train: 0.2611 acc_train: 0.9708 loss_val: 0.2550 acc_val: 0.9600 time: 0.0188s\n",
            "Epoch: 0054 loss_train: 0.2851 acc_train: 0.9375 loss_val: 0.2547 acc_val: 0.9633 time: 0.0185s\n",
            "Epoch: 0055 loss_train: 0.2752 acc_train: 0.9583 loss_val: 0.2544 acc_val: 0.9633 time: 0.0187s\n",
            "Epoch: 0056 loss_train: 0.3258 acc_train: 0.9417 loss_val: 0.2546 acc_val: 0.9600 time: 0.0178s\n",
            "Epoch: 0057 loss_train: 0.3081 acc_train: 0.9417 loss_val: 0.2549 acc_val: 0.9600 time: 0.0175s\n",
            "Epoch: 0058 loss_train: 0.2829 acc_train: 0.9542 loss_val: 0.2550 acc_val: 0.9633 time: 0.0211s\n",
            "Epoch: 0059 loss_train: 0.2958 acc_train: 0.9500 loss_val: 0.2553 acc_val: 0.9633 time: 0.0178s\n",
            "Epoch: 0060 loss_train: 0.2971 acc_train: 0.9458 loss_val: 0.2557 acc_val: 0.9633 time: 0.0192s\n",
            "Epoch: 0061 loss_train: 0.2853 acc_train: 0.9667 loss_val: 0.2559 acc_val: 0.9633 time: 0.0230s\n",
            "Epoch: 0062 loss_train: 0.2850 acc_train: 0.9542 loss_val: 0.2560 acc_val: 0.9600 time: 0.0181s\n",
            "Epoch: 0063 loss_train: 0.2743 acc_train: 0.9500 loss_val: 0.2561 acc_val: 0.9567 time: 0.0181s\n",
            "Epoch: 0064 loss_train: 0.2843 acc_train: 0.9583 loss_val: 0.2558 acc_val: 0.9567 time: 0.0176s\n",
            "Epoch: 0065 loss_train: 0.2898 acc_train: 0.9375 loss_val: 0.2553 acc_val: 0.9600 time: 0.0180s\n",
            "Epoch: 0066 loss_train: 0.2951 acc_train: 0.9542 loss_val: 0.2552 acc_val: 0.9633 time: 0.0178s\n",
            "Epoch: 0067 loss_train: 0.3035 acc_train: 0.9375 loss_val: 0.2557 acc_val: 0.9667 time: 0.0178s\n",
            "Epoch: 0068 loss_train: 0.2822 acc_train: 0.9500 loss_val: 0.2560 acc_val: 0.9667 time: 0.0270s\n",
            "Epoch: 0069 loss_train: 0.3058 acc_train: 0.9500 loss_val: 0.2560 acc_val: 0.9633 time: 0.0171s\n",
            "Epoch: 0070 loss_train: 0.2923 acc_train: 0.9542 loss_val: 0.2562 acc_val: 0.9600 time: 0.0189s\n",
            "Epoch: 0071 loss_train: 0.2958 acc_train: 0.9792 loss_val: 0.2564 acc_val: 0.9600 time: 0.0182s\n",
            "Epoch: 0072 loss_train: 0.2828 acc_train: 0.9542 loss_val: 0.2568 acc_val: 0.9600 time: 0.0182s\n",
            "Epoch: 0073 loss_train: 0.3107 acc_train: 0.9500 loss_val: 0.2578 acc_val: 0.9633 time: 0.0194s\n",
            "Epoch: 0074 loss_train: 0.2613 acc_train: 0.9667 loss_val: 0.2583 acc_val: 0.9633 time: 0.0168s\n",
            "Epoch: 0075 loss_train: 0.3154 acc_train: 0.9292 loss_val: 0.2580 acc_val: 0.9633 time: 0.0166s\n",
            "Epoch: 0076 loss_train: 0.2982 acc_train: 0.9417 loss_val: 0.2565 acc_val: 0.9633 time: 0.0177s\n",
            "Epoch: 0077 loss_train: 0.2974 acc_train: 0.9458 loss_val: 0.2559 acc_val: 0.9633 time: 0.0178s\n",
            "Epoch: 0078 loss_train: 0.3020 acc_train: 0.9417 loss_val: 0.2559 acc_val: 0.9667 time: 0.0187s\n",
            "Epoch: 0079 loss_train: 0.2908 acc_train: 0.9500 loss_val: 0.2560 acc_val: 0.9633 time: 0.0190s\n",
            "Epoch: 0080 loss_train: 0.2954 acc_train: 0.9583 loss_val: 0.2554 acc_val: 0.9633 time: 0.0175s\n",
            "Epoch: 0081 loss_train: 0.3015 acc_train: 0.9500 loss_val: 0.2547 acc_val: 0.9633 time: 0.0167s\n",
            "Epoch: 0082 loss_train: 0.2751 acc_train: 0.9625 loss_val: 0.2535 acc_val: 0.9633 time: 0.0191s\n",
            "Epoch: 0083 loss_train: 0.2894 acc_train: 0.9542 loss_val: 0.2529 acc_val: 0.9600 time: 0.0204s\n",
            "Epoch: 0084 loss_train: 0.2821 acc_train: 0.9542 loss_val: 0.2529 acc_val: 0.9600 time: 0.0199s\n",
            "Epoch: 0085 loss_train: 0.2829 acc_train: 0.9500 loss_val: 0.2530 acc_val: 0.9633 time: 0.0172s\n",
            "Epoch: 0086 loss_train: 0.2694 acc_train: 0.9750 loss_val: 0.2534 acc_val: 0.9633 time: 0.0181s\n",
            "Epoch: 0087 loss_train: 0.2940 acc_train: 0.9625 loss_val: 0.2539 acc_val: 0.9633 time: 0.0173s\n",
            "Epoch: 0088 loss_train: 0.3043 acc_train: 0.9458 loss_val: 0.2543 acc_val: 0.9633 time: 0.0287s\n",
            "Epoch: 0089 loss_train: 0.2578 acc_train: 0.9667 loss_val: 0.2546 acc_val: 0.9633 time: 0.0203s\n",
            "Epoch: 0090 loss_train: 0.2925 acc_train: 0.9583 loss_val: 0.2544 acc_val: 0.9633 time: 0.0174s\n",
            "Epoch: 0091 loss_train: 0.2944 acc_train: 0.9500 loss_val: 0.2540 acc_val: 0.9633 time: 0.0171s\n",
            "Epoch: 0092 loss_train: 0.2831 acc_train: 0.9500 loss_val: 0.2534 acc_val: 0.9600 time: 0.0169s\n",
            "Epoch: 0093 loss_train: 0.2511 acc_train: 0.9750 loss_val: 0.2528 acc_val: 0.9600 time: 0.0172s\n",
            "Epoch: 0094 loss_train: 0.3031 acc_train: 0.9458 loss_val: 0.2525 acc_val: 0.9633 time: 0.0197s\n",
            "Epoch: 0095 loss_train: 0.2688 acc_train: 0.9708 loss_val: 0.2526 acc_val: 0.9633 time: 0.0188s\n",
            "Epoch: 0096 loss_train: 0.2920 acc_train: 0.9542 loss_val: 0.2527 acc_val: 0.9633 time: 0.0182s\n",
            "Epoch: 0097 loss_train: 0.2742 acc_train: 0.9375 loss_val: 0.2528 acc_val: 0.9633 time: 0.0199s\n",
            "Epoch: 0098 loss_train: 0.2797 acc_train: 0.9500 loss_val: 0.2526 acc_val: 0.9633 time: 0.0255s\n",
            "Epoch: 0099 loss_train: 0.2955 acc_train: 0.9417 loss_val: 0.2520 acc_val: 0.9600 time: 0.0182s\n",
            "Epoch: 0100 loss_train: 0.2989 acc_train: 0.9500 loss_val: 0.2519 acc_val: 0.9600 time: 0.0175s\n",
            "Epoch: 0101 loss_train: 0.2694 acc_train: 0.9583 loss_val: 0.2525 acc_val: 0.9600 time: 0.0180s\n",
            "Epoch: 0102 loss_train: 0.2665 acc_train: 0.9750 loss_val: 0.2533 acc_val: 0.9600 time: 0.0177s\n",
            "Epoch: 0103 loss_train: 0.3026 acc_train: 0.9208 loss_val: 0.2539 acc_val: 0.9600 time: 0.0173s\n",
            "Epoch: 0104 loss_train: 0.2821 acc_train: 0.9417 loss_val: 0.2543 acc_val: 0.9600 time: 0.0169s\n",
            "Epoch: 0105 loss_train: 0.3256 acc_train: 0.9417 loss_val: 0.2541 acc_val: 0.9633 time: 0.0174s\n",
            "Epoch: 0106 loss_train: 0.2946 acc_train: 0.9417 loss_val: 0.2540 acc_val: 0.9633 time: 0.0173s\n",
            "Epoch: 0107 loss_train: 0.3089 acc_train: 0.9458 loss_val: 0.2540 acc_val: 0.9633 time: 0.0206s\n",
            "Epoch: 0108 loss_train: 0.2777 acc_train: 0.9583 loss_val: 0.2539 acc_val: 0.9633 time: 0.0212s\n",
            "Epoch: 0109 loss_train: 0.2678 acc_train: 0.9583 loss_val: 0.2536 acc_val: 0.9633 time: 0.0168s\n",
            "Epoch: 0110 loss_train: 0.2667 acc_train: 0.9417 loss_val: 0.2532 acc_val: 0.9633 time: 0.0170s\n",
            "Epoch: 0111 loss_train: 0.2719 acc_train: 0.9417 loss_val: 0.2529 acc_val: 0.9667 time: 0.0173s\n",
            "Epoch: 0112 loss_train: 0.2969 acc_train: 0.9458 loss_val: 0.2524 acc_val: 0.9667 time: 0.0166s\n",
            "Epoch: 0113 loss_train: 0.2988 acc_train: 0.9417 loss_val: 0.2513 acc_val: 0.9633 time: 0.0186s\n",
            "Epoch: 0114 loss_train: 0.2494 acc_train: 0.9667 loss_val: 0.2505 acc_val: 0.9633 time: 0.0165s\n",
            "Epoch: 0115 loss_train: 0.2838 acc_train: 0.9542 loss_val: 0.2503 acc_val: 0.9633 time: 0.0176s\n",
            "Epoch: 0116 loss_train: 0.2756 acc_train: 0.9625 loss_val: 0.2504 acc_val: 0.9667 time: 0.0167s\n",
            "Epoch: 0117 loss_train: 0.3032 acc_train: 0.9583 loss_val: 0.2503 acc_val: 0.9633 time: 0.0164s\n",
            "Epoch: 0118 loss_train: 0.2879 acc_train: 0.9542 loss_val: 0.2505 acc_val: 0.9633 time: 0.0168s\n",
            "Epoch: 0119 loss_train: 0.2867 acc_train: 0.9542 loss_val: 0.2509 acc_val: 0.9633 time: 0.0214s\n",
            "Epoch: 0120 loss_train: 0.2983 acc_train: 0.9417 loss_val: 0.2519 acc_val: 0.9633 time: 0.0200s\n",
            "Epoch: 0121 loss_train: 0.2653 acc_train: 0.9625 loss_val: 0.2527 acc_val: 0.9633 time: 0.0181s\n",
            "Epoch: 0122 loss_train: 0.2682 acc_train: 0.9417 loss_val: 0.2534 acc_val: 0.9633 time: 0.0174s\n",
            "Epoch: 0123 loss_train: 0.2734 acc_train: 0.9625 loss_val: 0.2536 acc_val: 0.9633 time: 0.0182s\n",
            "Epoch: 0124 loss_train: 0.3111 acc_train: 0.9417 loss_val: 0.2539 acc_val: 0.9633 time: 0.0191s\n",
            "Epoch: 0125 loss_train: 0.2809 acc_train: 0.9542 loss_val: 0.2534 acc_val: 0.9633 time: 0.0174s\n",
            "Epoch: 0126 loss_train: 0.3116 acc_train: 0.9500 loss_val: 0.2528 acc_val: 0.9633 time: 0.0173s\n",
            "Epoch: 0127 loss_train: 0.3076 acc_train: 0.9625 loss_val: 0.2526 acc_val: 0.9667 time: 0.0168s\n",
            "Epoch: 0128 loss_train: 0.2853 acc_train: 0.9542 loss_val: 0.2533 acc_val: 0.9667 time: 0.0153s\n",
            "Epoch: 0129 loss_train: 0.2902 acc_train: 0.9500 loss_val: 0.2540 acc_val: 0.9667 time: 0.0173s\n",
            "Epoch: 0130 loss_train: 0.2763 acc_train: 0.9667 loss_val: 0.2534 acc_val: 0.9633 time: 0.0223s\n",
            "Epoch: 0131 loss_train: 0.2891 acc_train: 0.9625 loss_val: 0.2517 acc_val: 0.9633 time: 0.0167s\n",
            "Epoch: 0132 loss_train: 0.2891 acc_train: 0.9542 loss_val: 0.2507 acc_val: 0.9633 time: 0.0198s\n",
            "Epoch: 0133 loss_train: 0.2776 acc_train: 0.9625 loss_val: 0.2512 acc_val: 0.9633 time: 0.0165s\n",
            "Epoch: 0134 loss_train: 0.2890 acc_train: 0.9417 loss_val: 0.2514 acc_val: 0.9633 time: 0.0167s\n",
            "Epoch: 0135 loss_train: 0.2885 acc_train: 0.9542 loss_val: 0.2508 acc_val: 0.9633 time: 0.0172s\n",
            "Epoch: 0136 loss_train: 0.2821 acc_train: 0.9625 loss_val: 0.2498 acc_val: 0.9633 time: 0.0164s\n",
            "Epoch: 0137 loss_train: 0.2844 acc_train: 0.9625 loss_val: 0.2493 acc_val: 0.9633 time: 0.0212s\n",
            "Epoch: 0138 loss_train: 0.2989 acc_train: 0.9583 loss_val: 0.2492 acc_val: 0.9667 time: 0.0311s\n",
            "Epoch: 0139 loss_train: 0.2842 acc_train: 0.9583 loss_val: 0.2491 acc_val: 0.9667 time: 0.0212s\n",
            "Epoch: 0140 loss_train: 0.2860 acc_train: 0.9417 loss_val: 0.2490 acc_val: 0.9667 time: 0.0185s\n",
            "Epoch: 0141 loss_train: 0.2634 acc_train: 0.9583 loss_val: 0.2489 acc_val: 0.9667 time: 0.0177s\n",
            "Epoch: 0142 loss_train: 0.2916 acc_train: 0.9458 loss_val: 0.2492 acc_val: 0.9633 time: 0.0180s\n",
            "Epoch: 0143 loss_train: 0.3082 acc_train: 0.9542 loss_val: 0.2496 acc_val: 0.9633 time: 0.0175s\n",
            "Epoch: 0144 loss_train: 0.2943 acc_train: 0.9667 loss_val: 0.2501 acc_val: 0.9633 time: 0.0172s\n",
            "Epoch: 0145 loss_train: 0.2904 acc_train: 0.9583 loss_val: 0.2500 acc_val: 0.9633 time: 0.0176s\n",
            "Epoch: 0146 loss_train: 0.3062 acc_train: 0.9583 loss_val: 0.2497 acc_val: 0.9633 time: 0.0176s\n",
            "Epoch: 0147 loss_train: 0.2981 acc_train: 0.9625 loss_val: 0.2491 acc_val: 0.9633 time: 0.0184s\n",
            "Epoch: 0148 loss_train: 0.3012 acc_train: 0.9625 loss_val: 0.2486 acc_val: 0.9633 time: 0.0178s\n",
            "Epoch: 0149 loss_train: 0.2920 acc_train: 0.9667 loss_val: 0.2481 acc_val: 0.9633 time: 0.0210s\n",
            "Epoch: 0150 loss_train: 0.2943 acc_train: 0.9417 loss_val: 0.2474 acc_val: 0.9633 time: 0.0175s\n",
            "Epoch: 0151 loss_train: 0.2675 acc_train: 0.9542 loss_val: 0.2468 acc_val: 0.9633 time: 0.0179s\n",
            "Epoch: 0152 loss_train: 0.2500 acc_train: 0.9625 loss_val: 0.2466 acc_val: 0.9633 time: 0.0167s\n",
            "Epoch: 0153 loss_train: 0.2919 acc_train: 0.9542 loss_val: 0.2462 acc_val: 0.9633 time: 0.0177s\n",
            "Epoch: 0154 loss_train: 0.3195 acc_train: 0.9458 loss_val: 0.2462 acc_val: 0.9633 time: 0.0173s\n",
            "Epoch: 0155 loss_train: 0.3176 acc_train: 0.9375 loss_val: 0.2466 acc_val: 0.9633 time: 0.0174s\n",
            "Epoch: 0156 loss_train: 0.3011 acc_train: 0.9458 loss_val: 0.2472 acc_val: 0.9633 time: 0.0199s\n",
            "Epoch: 0157 loss_train: 0.2641 acc_train: 0.9542 loss_val: 0.2476 acc_val: 0.9633 time: 0.0184s\n",
            "Epoch: 0158 loss_train: 0.2732 acc_train: 0.9583 loss_val: 0.2477 acc_val: 0.9633 time: 0.0179s\n",
            "Epoch: 0159 loss_train: 0.2878 acc_train: 0.9542 loss_val: 0.2476 acc_val: 0.9633 time: 0.0207s\n",
            "Epoch: 0160 loss_train: 0.2781 acc_train: 0.9458 loss_val: 0.2475 acc_val: 0.9633 time: 0.0173s\n",
            "Epoch: 0161 loss_train: 0.2939 acc_train: 0.9500 loss_val: 0.2480 acc_val: 0.9633 time: 0.0174s\n",
            "Epoch: 0162 loss_train: 0.2708 acc_train: 0.9500 loss_val: 0.2486 acc_val: 0.9633 time: 0.0173s\n",
            "Epoch: 0163 loss_train: 0.2705 acc_train: 0.9625 loss_val: 0.2485 acc_val: 0.9633 time: 0.0177s\n",
            "Epoch: 0164 loss_train: 0.2887 acc_train: 0.9542 loss_val: 0.2482 acc_val: 0.9633 time: 0.0178s\n",
            "Epoch: 0165 loss_train: 0.3017 acc_train: 0.9500 loss_val: 0.2478 acc_val: 0.9633 time: 0.0169s\n",
            "Epoch: 0166 loss_train: 0.2795 acc_train: 0.9500 loss_val: 0.2479 acc_val: 0.9667 time: 0.0180s\n",
            "Epoch: 0167 loss_train: 0.2659 acc_train: 0.9667 loss_val: 0.2484 acc_val: 0.9667 time: 0.0202s\n",
            "Epoch: 0168 loss_train: 0.2958 acc_train: 0.9333 loss_val: 0.2490 acc_val: 0.9667 time: 0.0181s\n",
            "Epoch: 0169 loss_train: 0.3067 acc_train: 0.9375 loss_val: 0.2487 acc_val: 0.9633 time: 0.0187s\n",
            "Epoch: 0170 loss_train: 0.2792 acc_train: 0.9500 loss_val: 0.2481 acc_val: 0.9667 time: 0.0231s\n",
            "Epoch: 0171 loss_train: 0.2873 acc_train: 0.9458 loss_val: 0.2480 acc_val: 0.9633 time: 0.0170s\n",
            "Epoch: 0172 loss_train: 0.2769 acc_train: 0.9542 loss_val: 0.2484 acc_val: 0.9633 time: 0.0181s\n",
            "Epoch: 0173 loss_train: 0.2772 acc_train: 0.9458 loss_val: 0.2488 acc_val: 0.9633 time: 0.0174s\n",
            "Epoch: 0174 loss_train: 0.3060 acc_train: 0.9375 loss_val: 0.2485 acc_val: 0.9633 time: 0.0164s\n",
            "Epoch: 0175 loss_train: 0.2624 acc_train: 0.9708 loss_val: 0.2479 acc_val: 0.9667 time: 0.0180s\n",
            "Epoch: 0176 loss_train: 0.2912 acc_train: 0.9542 loss_val: 0.2466 acc_val: 0.9667 time: 0.0170s\n",
            "Epoch: 0177 loss_train: 0.2698 acc_train: 0.9625 loss_val: 0.2458 acc_val: 0.9667 time: 0.0180s\n",
            "Epoch: 0178 loss_train: 0.2575 acc_train: 0.9667 loss_val: 0.2457 acc_val: 0.9667 time: 0.0175s\n",
            "Epoch: 0179 loss_train: 0.2487 acc_train: 0.9750 loss_val: 0.2462 acc_val: 0.9667 time: 0.0247s\n",
            "Epoch: 0180 loss_train: 0.2780 acc_train: 0.9583 loss_val: 0.2464 acc_val: 0.9667 time: 0.0182s\n",
            "Epoch: 0181 loss_train: 0.2472 acc_train: 0.9750 loss_val: 0.2461 acc_val: 0.9667 time: 0.0174s\n",
            "Epoch: 0182 loss_train: 0.2646 acc_train: 0.9542 loss_val: 0.2459 acc_val: 0.9667 time: 0.0177s\n",
            "Epoch: 0183 loss_train: 0.2803 acc_train: 0.9500 loss_val: 0.2459 acc_val: 0.9633 time: 0.0166s\n",
            "Epoch: 0184 loss_train: 0.2762 acc_train: 0.9667 loss_val: 0.2462 acc_val: 0.9633 time: 0.0168s\n",
            "Epoch: 0185 loss_train: 0.3042 acc_train: 0.9417 loss_val: 0.2463 acc_val: 0.9667 time: 0.0233s\n",
            "Epoch: 0186 loss_train: 0.2573 acc_train: 0.9625 loss_val: 0.2463 acc_val: 0.9667 time: 0.0188s\n",
            "Epoch: 0187 loss_train: 0.2948 acc_train: 0.9500 loss_val: 0.2463 acc_val: 0.9667 time: 0.0167s\n",
            "Epoch: 0188 loss_train: 0.2920 acc_train: 0.9583 loss_val: 0.2465 acc_val: 0.9667 time: 0.0172s\n",
            "Epoch: 0189 loss_train: 0.2980 acc_train: 0.9417 loss_val: 0.2468 acc_val: 0.9667 time: 0.0213s\n",
            "Epoch: 0190 loss_train: 0.3102 acc_train: 0.9333 loss_val: 0.2468 acc_val: 0.9667 time: 0.0200s\n",
            "Epoch: 0191 loss_train: 0.2835 acc_train: 0.9500 loss_val: 0.2465 acc_val: 0.9667 time: 0.0176s\n",
            "Epoch: 0192 loss_train: 0.2665 acc_train: 0.9708 loss_val: 0.2465 acc_val: 0.9667 time: 0.0178s\n",
            "Epoch: 0193 loss_train: 0.2789 acc_train: 0.9500 loss_val: 0.2464 acc_val: 0.9633 time: 0.0193s\n",
            "Epoch: 0194 loss_train: 0.2585 acc_train: 0.9667 loss_val: 0.2462 acc_val: 0.9633 time: 0.0183s\n",
            "Epoch: 0195 loss_train: 0.2736 acc_train: 0.9500 loss_val: 0.2458 acc_val: 0.9633 time: 0.0182s\n",
            "Epoch: 0196 loss_train: 0.2775 acc_train: 0.9375 loss_val: 0.2455 acc_val: 0.9633 time: 0.0183s\n",
            "Epoch: 0197 loss_train: 0.2756 acc_train: 0.9500 loss_val: 0.2447 acc_val: 0.9633 time: 0.0172s\n",
            "Epoch: 0198 loss_train: 0.2799 acc_train: 0.9500 loss_val: 0.2445 acc_val: 0.9633 time: 0.0162s\n",
            "Epoch: 0199 loss_train: 0.2573 acc_train: 0.9625 loss_val: 0.2445 acc_val: 0.9667 time: 0.0197s\n",
            "Epoch: 0200 loss_train: 0.2635 acc_train: 0.9542 loss_val: 0.2445 acc_val: 0.9633 time: 0.0178s\n",
            "Optimization Finished for 240 labeled nodes!\n",
            "Total time elapsed: 4.2479s\n",
            "Test set results: loss= 0.5588 accuracy= 0.8240\n",
            "Epoch: 0001 loss_train: 0.3397 acc_train: 0.9167 loss_val: 0.2431 acc_val: 0.9633 time: 0.0171s\n",
            "Epoch: 0002 loss_train: 0.3053 acc_train: 0.9400 loss_val: 0.2413 acc_val: 0.9667 time: 0.0246s\n",
            "Epoch: 0003 loss_train: 0.3390 acc_train: 0.9433 loss_val: 0.2399 acc_val: 0.9667 time: 0.0201s\n",
            "Epoch: 0004 loss_train: 0.3210 acc_train: 0.9333 loss_val: 0.2390 acc_val: 0.9667 time: 0.0162s\n",
            "Epoch: 0005 loss_train: 0.3025 acc_train: 0.9367 loss_val: 0.2372 acc_val: 0.9667 time: 0.0164s\n",
            "Epoch: 0006 loss_train: 0.3414 acc_train: 0.9400 loss_val: 0.2344 acc_val: 0.9667 time: 0.0166s\n",
            "Epoch: 0007 loss_train: 0.3344 acc_train: 0.9300 loss_val: 0.2316 acc_val: 0.9633 time: 0.0166s\n",
            "Epoch: 0008 loss_train: 0.3301 acc_train: 0.9267 loss_val: 0.2302 acc_val: 0.9633 time: 0.0172s\n",
            "Epoch: 0009 loss_train: 0.3075 acc_train: 0.9367 loss_val: 0.2293 acc_val: 0.9633 time: 0.0203s\n",
            "Epoch: 0010 loss_train: 0.3187 acc_train: 0.9400 loss_val: 0.2279 acc_val: 0.9600 time: 0.0172s\n",
            "Epoch: 0011 loss_train: 0.2967 acc_train: 0.9567 loss_val: 0.2254 acc_val: 0.9600 time: 0.0179s\n",
            "Epoch: 0012 loss_train: 0.3018 acc_train: 0.9367 loss_val: 0.2220 acc_val: 0.9633 time: 0.0154s\n",
            "Epoch: 0013 loss_train: 0.3066 acc_train: 0.9400 loss_val: 0.2190 acc_val: 0.9633 time: 0.0257s\n",
            "Epoch: 0014 loss_train: 0.3188 acc_train: 0.9467 loss_val: 0.2166 acc_val: 0.9600 time: 0.0297s\n",
            "Epoch: 0015 loss_train: 0.2863 acc_train: 0.9567 loss_val: 0.2149 acc_val: 0.9600 time: 0.0266s\n",
            "Epoch: 0016 loss_train: 0.2745 acc_train: 0.9633 loss_val: 0.2133 acc_val: 0.9633 time: 0.0254s\n",
            "Epoch: 0017 loss_train: 0.3303 acc_train: 0.9467 loss_val: 0.2118 acc_val: 0.9700 time: 0.0261s\n",
            "Epoch: 0018 loss_train: 0.2755 acc_train: 0.9500 loss_val: 0.2104 acc_val: 0.9700 time: 0.0282s\n",
            "Epoch: 0019 loss_train: 0.2933 acc_train: 0.9533 loss_val: 0.2095 acc_val: 0.9700 time: 0.0247s\n",
            "Epoch: 0020 loss_train: 0.2912 acc_train: 0.9433 loss_val: 0.2093 acc_val: 0.9700 time: 0.0254s\n",
            "Epoch: 0021 loss_train: 0.2795 acc_train: 0.9467 loss_val: 0.2098 acc_val: 0.9700 time: 0.0247s\n",
            "Epoch: 0022 loss_train: 0.2698 acc_train: 0.9500 loss_val: 0.2103 acc_val: 0.9633 time: 0.0285s\n",
            "Epoch: 0023 loss_train: 0.2838 acc_train: 0.9333 loss_val: 0.2109 acc_val: 0.9600 time: 0.0256s\n",
            "Epoch: 0024 loss_train: 0.3035 acc_train: 0.9433 loss_val: 0.2103 acc_val: 0.9633 time: 0.0248s\n",
            "Epoch: 0025 loss_train: 0.2704 acc_train: 0.9467 loss_val: 0.2094 acc_val: 0.9633 time: 0.0269s\n",
            "Epoch: 0026 loss_train: 0.2676 acc_train: 0.9467 loss_val: 0.2088 acc_val: 0.9633 time: 0.0233s\n",
            "Epoch: 0027 loss_train: 0.2890 acc_train: 0.9433 loss_val: 0.2086 acc_val: 0.9667 time: 0.0245s\n",
            "Epoch: 0028 loss_train: 0.3043 acc_train: 0.9400 loss_val: 0.2089 acc_val: 0.9700 time: 0.0304s\n",
            "Epoch: 0029 loss_train: 0.2984 acc_train: 0.9433 loss_val: 0.2089 acc_val: 0.9733 time: 0.0256s\n",
            "Epoch: 0030 loss_train: 0.3037 acc_train: 0.9267 loss_val: 0.2088 acc_val: 0.9733 time: 0.0254s\n",
            "Epoch: 0031 loss_train: 0.2753 acc_train: 0.9433 loss_val: 0.2088 acc_val: 0.9733 time: 0.0317s\n",
            "Epoch: 0032 loss_train: 0.2929 acc_train: 0.9367 loss_val: 0.2089 acc_val: 0.9733 time: 0.0269s\n",
            "Epoch: 0033 loss_train: 0.2713 acc_train: 0.9567 loss_val: 0.2093 acc_val: 0.9733 time: 0.0245s\n",
            "Epoch: 0034 loss_train: 0.2712 acc_train: 0.9567 loss_val: 0.2095 acc_val: 0.9767 time: 0.0248s\n",
            "Epoch: 0035 loss_train: 0.2771 acc_train: 0.9500 loss_val: 0.2095 acc_val: 0.9767 time: 0.0244s\n",
            "Epoch: 0036 loss_train: 0.3161 acc_train: 0.9533 loss_val: 0.2095 acc_val: 0.9800 time: 0.0280s\n",
            "Epoch: 0037 loss_train: 0.2839 acc_train: 0.9267 loss_val: 0.2091 acc_val: 0.9733 time: 0.0252s\n",
            "Epoch: 0038 loss_train: 0.2774 acc_train: 0.9500 loss_val: 0.2091 acc_val: 0.9733 time: 0.0248s\n",
            "Epoch: 0039 loss_train: 0.3118 acc_train: 0.9400 loss_val: 0.2092 acc_val: 0.9700 time: 0.0264s\n",
            "Epoch: 0040 loss_train: 0.2997 acc_train: 0.9500 loss_val: 0.2091 acc_val: 0.9767 time: 0.0282s\n",
            "Epoch: 0041 loss_train: 0.2691 acc_train: 0.9633 loss_val: 0.2087 acc_val: 0.9767 time: 0.0250s\n",
            "Epoch: 0042 loss_train: 0.2923 acc_train: 0.9533 loss_val: 0.2085 acc_val: 0.9800 time: 0.0275s\n",
            "Epoch: 0043 loss_train: 0.3040 acc_train: 0.9400 loss_val: 0.2089 acc_val: 0.9767 time: 0.0298s\n",
            "Epoch: 0044 loss_train: 0.2877 acc_train: 0.9567 loss_val: 0.2094 acc_val: 0.9733 time: 0.0252s\n",
            "Epoch: 0045 loss_train: 0.2905 acc_train: 0.9467 loss_val: 0.2091 acc_val: 0.9767 time: 0.0282s\n",
            "Epoch: 0046 loss_train: 0.3011 acc_train: 0.9567 loss_val: 0.2089 acc_val: 0.9700 time: 0.0248s\n",
            "Epoch: 0047 loss_train: 0.3078 acc_train: 0.9333 loss_val: 0.2087 acc_val: 0.9700 time: 0.0249s\n",
            "Epoch: 0048 loss_train: 0.2987 acc_train: 0.9533 loss_val: 0.2082 acc_val: 0.9733 time: 0.0259s\n",
            "Epoch: 0049 loss_train: 0.2935 acc_train: 0.9367 loss_val: 0.2079 acc_val: 0.9767 time: 0.0252s\n",
            "Epoch: 0050 loss_train: 0.2782 acc_train: 0.9500 loss_val: 0.2080 acc_val: 0.9767 time: 0.0260s\n",
            "Epoch: 0051 loss_train: 0.3018 acc_train: 0.9400 loss_val: 0.2080 acc_val: 0.9733 time: 0.0250s\n",
            "Epoch: 0052 loss_train: 0.3154 acc_train: 0.9367 loss_val: 0.2081 acc_val: 0.9733 time: 0.0252s\n",
            "Epoch: 0053 loss_train: 0.3185 acc_train: 0.9400 loss_val: 0.2082 acc_val: 0.9800 time: 0.0268s\n",
            "Epoch: 0054 loss_train: 0.2864 acc_train: 0.9533 loss_val: 0.2082 acc_val: 0.9800 time: 0.0248s\n",
            "Epoch: 0055 loss_train: 0.2521 acc_train: 0.9600 loss_val: 0.2081 acc_val: 0.9800 time: 0.0265s\n",
            "Epoch: 0056 loss_train: 0.2979 acc_train: 0.9400 loss_val: 0.2083 acc_val: 0.9800 time: 0.0252s\n",
            "Epoch: 0057 loss_train: 0.3220 acc_train: 0.9333 loss_val: 0.2086 acc_val: 0.9767 time: 0.0261s\n",
            "Epoch: 0058 loss_train: 0.2978 acc_train: 0.9433 loss_val: 0.2090 acc_val: 0.9767 time: 0.0249s\n",
            "Epoch: 0059 loss_train: 0.2919 acc_train: 0.9433 loss_val: 0.2093 acc_val: 0.9700 time: 0.0249s\n",
            "Epoch: 0060 loss_train: 0.3171 acc_train: 0.9433 loss_val: 0.2092 acc_val: 0.9700 time: 0.0256s\n",
            "Epoch: 0061 loss_train: 0.3310 acc_train: 0.9433 loss_val: 0.2092 acc_val: 0.9733 time: 0.0263s\n",
            "Epoch: 0062 loss_train: 0.2671 acc_train: 0.9700 loss_val: 0.2092 acc_val: 0.9700 time: 0.0263s\n",
            "Epoch: 0063 loss_train: 0.3061 acc_train: 0.9333 loss_val: 0.2090 acc_val: 0.9667 time: 0.0237s\n",
            "Epoch: 0064 loss_train: 0.2857 acc_train: 0.9433 loss_val: 0.2087 acc_val: 0.9667 time: 0.0277s\n",
            "Epoch: 0065 loss_train: 0.2753 acc_train: 0.9500 loss_val: 0.2083 acc_val: 0.9700 time: 0.0252s\n",
            "Epoch: 0066 loss_train: 0.2978 acc_train: 0.9467 loss_val: 0.2083 acc_val: 0.9633 time: 0.0249s\n",
            "Epoch: 0067 loss_train: 0.2845 acc_train: 0.9533 loss_val: 0.2086 acc_val: 0.9667 time: 0.0264s\n",
            "Epoch: 0068 loss_train: 0.3090 acc_train: 0.9367 loss_val: 0.2089 acc_val: 0.9667 time: 0.0288s\n",
            "Epoch: 0069 loss_train: 0.2749 acc_train: 0.9533 loss_val: 0.2086 acc_val: 0.9667 time: 0.0266s\n",
            "Epoch: 0070 loss_train: 0.2705 acc_train: 0.9533 loss_val: 0.2080 acc_val: 0.9667 time: 0.0245s\n",
            "Epoch: 0071 loss_train: 0.3037 acc_train: 0.9233 loss_val: 0.2078 acc_val: 0.9767 time: 0.0238s\n",
            "Epoch: 0072 loss_train: 0.2837 acc_train: 0.9500 loss_val: 0.2077 acc_val: 0.9700 time: 0.0240s\n",
            "Epoch: 0073 loss_train: 0.2790 acc_train: 0.9567 loss_val: 0.2077 acc_val: 0.9733 time: 0.0250s\n",
            "Epoch: 0074 loss_train: 0.2646 acc_train: 0.9600 loss_val: 0.2079 acc_val: 0.9733 time: 0.0262s\n",
            "Epoch: 0075 loss_train: 0.2732 acc_train: 0.9533 loss_val: 0.2081 acc_val: 0.9733 time: 0.0333s\n",
            "Epoch: 0076 loss_train: 0.2757 acc_train: 0.9667 loss_val: 0.2078 acc_val: 0.9667 time: 0.0272s\n",
            "Epoch: 0077 loss_train: 0.3111 acc_train: 0.9300 loss_val: 0.2078 acc_val: 0.9700 time: 0.0242s\n",
            "Epoch: 0078 loss_train: 0.2942 acc_train: 0.9433 loss_val: 0.2087 acc_val: 0.9700 time: 0.0244s\n",
            "Epoch: 0079 loss_train: 0.3222 acc_train: 0.9367 loss_val: 0.2098 acc_val: 0.9700 time: 0.0277s\n",
            "Epoch: 0080 loss_train: 0.3051 acc_train: 0.9467 loss_val: 0.2093 acc_val: 0.9700 time: 0.0242s\n",
            "Epoch: 0081 loss_train: 0.2774 acc_train: 0.9400 loss_val: 0.2079 acc_val: 0.9700 time: 0.0259s\n",
            "Epoch: 0082 loss_train: 0.2682 acc_train: 0.9567 loss_val: 0.2069 acc_val: 0.9700 time: 0.0252s\n",
            "Epoch: 0083 loss_train: 0.2741 acc_train: 0.9567 loss_val: 0.2064 acc_val: 0.9700 time: 0.0279s\n",
            "Epoch: 0084 loss_train: 0.3004 acc_train: 0.9533 loss_val: 0.2066 acc_val: 0.9733 time: 0.0281s\n",
            "Epoch: 0085 loss_train: 0.2810 acc_train: 0.9433 loss_val: 0.2070 acc_val: 0.9733 time: 0.0251s\n",
            "Epoch: 0086 loss_train: 0.3067 acc_train: 0.9433 loss_val: 0.2071 acc_val: 0.9733 time: 0.0250s\n",
            "Epoch: 0087 loss_train: 0.2849 acc_train: 0.9533 loss_val: 0.2067 acc_val: 0.9733 time: 0.0256s\n",
            "Epoch: 0088 loss_train: 0.2901 acc_train: 0.9567 loss_val: 0.2067 acc_val: 0.9700 time: 0.0256s\n",
            "Epoch: 0089 loss_train: 0.3054 acc_train: 0.9300 loss_val: 0.2074 acc_val: 0.9700 time: 0.0276s\n",
            "Epoch: 0090 loss_train: 0.2741 acc_train: 0.9500 loss_val: 0.2088 acc_val: 0.9700 time: 0.0277s\n",
            "Epoch: 0091 loss_train: 0.2892 acc_train: 0.9500 loss_val: 0.2093 acc_val: 0.9667 time: 0.0259s\n",
            "Epoch: 0092 loss_train: 0.2872 acc_train: 0.9500 loss_val: 0.2085 acc_val: 0.9700 time: 0.0280s\n",
            "Epoch: 0093 loss_train: 0.3048 acc_train: 0.9400 loss_val: 0.2077 acc_val: 0.9700 time: 0.0256s\n",
            "Epoch: 0094 loss_train: 0.2922 acc_train: 0.9367 loss_val: 0.2069 acc_val: 0.9733 time: 0.0297s\n",
            "Epoch: 0095 loss_train: 0.2778 acc_train: 0.9533 loss_val: 0.2069 acc_val: 0.9733 time: 0.0261s\n",
            "Epoch: 0096 loss_train: 0.2963 acc_train: 0.9500 loss_val: 0.2072 acc_val: 0.9733 time: 0.0327s\n",
            "Epoch: 0097 loss_train: 0.2967 acc_train: 0.9467 loss_val: 0.2076 acc_val: 0.9733 time: 0.0253s\n",
            "Epoch: 0098 loss_train: 0.2829 acc_train: 0.9600 loss_val: 0.2073 acc_val: 0.9733 time: 0.0261s\n",
            "Epoch: 0099 loss_train: 0.2676 acc_train: 0.9500 loss_val: 0.2070 acc_val: 0.9733 time: 0.0270s\n",
            "Epoch: 0100 loss_train: 0.2859 acc_train: 0.9533 loss_val: 0.2074 acc_val: 0.9700 time: 0.0261s\n",
            "Epoch: 0101 loss_train: 0.2721 acc_train: 0.9600 loss_val: 0.2087 acc_val: 0.9700 time: 0.0258s\n",
            "Epoch: 0102 loss_train: 0.2880 acc_train: 0.9567 loss_val: 0.2094 acc_val: 0.9700 time: 0.0254s\n",
            "Epoch: 0103 loss_train: 0.2740 acc_train: 0.9633 loss_val: 0.2095 acc_val: 0.9700 time: 0.0294s\n",
            "Epoch: 0104 loss_train: 0.2831 acc_train: 0.9533 loss_val: 0.2086 acc_val: 0.9700 time: 0.0280s\n",
            "Epoch: 0105 loss_train: 0.2829 acc_train: 0.9467 loss_val: 0.2076 acc_val: 0.9700 time: 0.0254s\n",
            "Epoch: 0106 loss_train: 0.2557 acc_train: 0.9533 loss_val: 0.2065 acc_val: 0.9700 time: 0.0265s\n",
            "Epoch: 0107 loss_train: 0.2859 acc_train: 0.9433 loss_val: 0.2064 acc_val: 0.9767 time: 0.0258s\n",
            "Epoch: 0108 loss_train: 0.2936 acc_train: 0.9533 loss_val: 0.2068 acc_val: 0.9733 time: 0.0263s\n",
            "Epoch: 0109 loss_train: 0.2867 acc_train: 0.9300 loss_val: 0.2073 acc_val: 0.9733 time: 0.0250s\n",
            "Epoch: 0110 loss_train: 0.2891 acc_train: 0.9533 loss_val: 0.2075 acc_val: 0.9733 time: 0.0295s\n",
            "Epoch: 0111 loss_train: 0.3016 acc_train: 0.9400 loss_val: 0.2079 acc_val: 0.9700 time: 0.0245s\n",
            "Epoch: 0112 loss_train: 0.2762 acc_train: 0.9533 loss_val: 0.2085 acc_val: 0.9733 time: 0.0246s\n",
            "Epoch: 0113 loss_train: 0.2894 acc_train: 0.9367 loss_val: 0.2090 acc_val: 0.9700 time: 0.0248s\n",
            "Epoch: 0114 loss_train: 0.2735 acc_train: 0.9600 loss_val: 0.2090 acc_val: 0.9700 time: 0.0256s\n",
            "Epoch: 0115 loss_train: 0.2785 acc_train: 0.9500 loss_val: 0.2087 acc_val: 0.9733 time: 0.0248s\n",
            "Epoch: 0116 loss_train: 0.2865 acc_train: 0.9400 loss_val: 0.2085 acc_val: 0.9700 time: 0.0243s\n",
            "Epoch: 0117 loss_train: 0.2907 acc_train: 0.9567 loss_val: 0.2090 acc_val: 0.9700 time: 0.0245s\n",
            "Epoch: 0118 loss_train: 0.2688 acc_train: 0.9500 loss_val: 0.2085 acc_val: 0.9700 time: 0.0303s\n",
            "Epoch: 0119 loss_train: 0.2963 acc_train: 0.9333 loss_val: 0.2076 acc_val: 0.9733 time: 0.0243s\n",
            "Epoch: 0120 loss_train: 0.3193 acc_train: 0.9267 loss_val: 0.2069 acc_val: 0.9700 time: 0.0249s\n",
            "Epoch: 0121 loss_train: 0.2979 acc_train: 0.9433 loss_val: 0.2071 acc_val: 0.9733 time: 0.0239s\n",
            "Epoch: 0122 loss_train: 0.2948 acc_train: 0.9467 loss_val: 0.2079 acc_val: 0.9733 time: 0.0239s\n",
            "Epoch: 0123 loss_train: 0.3016 acc_train: 0.9433 loss_val: 0.2077 acc_val: 0.9767 time: 0.0238s\n",
            "Epoch: 0124 loss_train: 0.2794 acc_train: 0.9567 loss_val: 0.2075 acc_val: 0.9767 time: 0.0243s\n",
            "Epoch: 0125 loss_train: 0.2791 acc_train: 0.9533 loss_val: 0.2068 acc_val: 0.9767 time: 0.0267s\n",
            "Epoch: 0126 loss_train: 0.2882 acc_train: 0.9433 loss_val: 0.2058 acc_val: 0.9767 time: 0.0267s\n",
            "Epoch: 0127 loss_train: 0.2968 acc_train: 0.9500 loss_val: 0.2052 acc_val: 0.9767 time: 0.0261s\n",
            "Epoch: 0128 loss_train: 0.2855 acc_train: 0.9333 loss_val: 0.2052 acc_val: 0.9767 time: 0.0258s\n",
            "Epoch: 0129 loss_train: 0.2856 acc_train: 0.9533 loss_val: 0.2054 acc_val: 0.9700 time: 0.0270s\n",
            "Epoch: 0130 loss_train: 0.2607 acc_train: 0.9567 loss_val: 0.2058 acc_val: 0.9733 time: 0.0258s\n",
            "Epoch: 0131 loss_train: 0.2895 acc_train: 0.9433 loss_val: 0.2064 acc_val: 0.9700 time: 0.0253s\n",
            "Epoch: 0132 loss_train: 0.2476 acc_train: 0.9667 loss_val: 0.2064 acc_val: 0.9700 time: 0.0260s\n",
            "Epoch: 0133 loss_train: 0.3104 acc_train: 0.9367 loss_val: 0.2065 acc_val: 0.9733 time: 0.0272s\n",
            "Epoch: 0134 loss_train: 0.2687 acc_train: 0.9567 loss_val: 0.2066 acc_val: 0.9733 time: 0.0258s\n",
            "Epoch: 0135 loss_train: 0.2743 acc_train: 0.9567 loss_val: 0.2068 acc_val: 0.9667 time: 0.0264s\n",
            "Epoch: 0136 loss_train: 0.2941 acc_train: 0.9533 loss_val: 0.2071 acc_val: 0.9667 time: 0.0259s\n",
            "Epoch: 0137 loss_train: 0.3139 acc_train: 0.9567 loss_val: 0.2078 acc_val: 0.9667 time: 0.0247s\n",
            "Epoch: 0138 loss_train: 0.3027 acc_train: 0.9333 loss_val: 0.2075 acc_val: 0.9667 time: 0.0259s\n",
            "Epoch: 0139 loss_train: 0.2654 acc_train: 0.9533 loss_val: 0.2067 acc_val: 0.9767 time: 0.0252s\n",
            "Epoch: 0140 loss_train: 0.2978 acc_train: 0.9400 loss_val: 0.2063 acc_val: 0.9800 time: 0.0270s\n",
            "Epoch: 0141 loss_train: 0.3076 acc_train: 0.9233 loss_val: 0.2069 acc_val: 0.9733 time: 0.0261s\n",
            "Epoch: 0142 loss_train: 0.2891 acc_train: 0.9433 loss_val: 0.2074 acc_val: 0.9733 time: 0.0254s\n",
            "Epoch: 0143 loss_train: 0.2796 acc_train: 0.9533 loss_val: 0.2072 acc_val: 0.9733 time: 0.0264s\n",
            "Epoch: 0144 loss_train: 0.3062 acc_train: 0.9433 loss_val: 0.2068 acc_val: 0.9700 time: 0.0255s\n",
            "Epoch: 0145 loss_train: 0.2505 acc_train: 0.9533 loss_val: 0.2072 acc_val: 0.9733 time: 0.0305s\n",
            "Epoch: 0146 loss_train: 0.2677 acc_train: 0.9533 loss_val: 0.2076 acc_val: 0.9733 time: 0.0260s\n",
            "Epoch: 0147 loss_train: 0.3039 acc_train: 0.9467 loss_val: 0.2079 acc_val: 0.9733 time: 0.0279s\n",
            "Epoch: 0148 loss_train: 0.3045 acc_train: 0.9500 loss_val: 0.2071 acc_val: 0.9700 time: 0.0249s\n",
            "Epoch: 0149 loss_train: 0.2831 acc_train: 0.9533 loss_val: 0.2058 acc_val: 0.9700 time: 0.0267s\n",
            "Epoch: 0150 loss_train: 0.2825 acc_train: 0.9567 loss_val: 0.2057 acc_val: 0.9700 time: 0.0279s\n",
            "Epoch: 0151 loss_train: 0.3056 acc_train: 0.9400 loss_val: 0.2061 acc_val: 0.9733 time: 0.0246s\n",
            "Epoch: 0152 loss_train: 0.3025 acc_train: 0.9433 loss_val: 0.2065 acc_val: 0.9733 time: 0.0255s\n",
            "Epoch: 0153 loss_train: 0.2778 acc_train: 0.9633 loss_val: 0.2068 acc_val: 0.9733 time: 0.0240s\n",
            "Epoch: 0154 loss_train: 0.2761 acc_train: 0.9367 loss_val: 0.2071 acc_val: 0.9733 time: 0.0252s\n",
            "Epoch: 0155 loss_train: 0.2539 acc_train: 0.9700 loss_val: 0.2075 acc_val: 0.9700 time: 0.0259s\n",
            "Epoch: 0156 loss_train: 0.2708 acc_train: 0.9500 loss_val: 0.2076 acc_val: 0.9700 time: 0.0268s\n",
            "Epoch: 0157 loss_train: 0.3040 acc_train: 0.9367 loss_val: 0.2071 acc_val: 0.9700 time: 0.0217s\n",
            "Epoch: 0158 loss_train: 0.2977 acc_train: 0.9500 loss_val: 0.2067 acc_val: 0.9700 time: 0.0185s\n",
            "Epoch: 0159 loss_train: 0.3143 acc_train: 0.9367 loss_val: 0.2066 acc_val: 0.9733 time: 0.0195s\n",
            "Epoch: 0160 loss_train: 0.2850 acc_train: 0.9633 loss_val: 0.2070 acc_val: 0.9733 time: 0.0229s\n",
            "Epoch: 0161 loss_train: 0.2697 acc_train: 0.9533 loss_val: 0.2070 acc_val: 0.9733 time: 0.0179s\n",
            "Epoch: 0162 loss_train: 0.2723 acc_train: 0.9600 loss_val: 0.2063 acc_val: 0.9733 time: 0.0182s\n",
            "Epoch: 0163 loss_train: 0.2726 acc_train: 0.9700 loss_val: 0.2054 acc_val: 0.9700 time: 0.0179s\n",
            "Epoch: 0164 loss_train: 0.3010 acc_train: 0.9333 loss_val: 0.2053 acc_val: 0.9700 time: 0.0195s\n",
            "Epoch: 0165 loss_train: 0.2912 acc_train: 0.9400 loss_val: 0.2053 acc_val: 0.9733 time: 0.0222s\n",
            "Epoch: 0166 loss_train: 0.2875 acc_train: 0.9433 loss_val: 0.2057 acc_val: 0.9767 time: 0.0185s\n",
            "Epoch: 0167 loss_train: 0.2946 acc_train: 0.9300 loss_val: 0.2055 acc_val: 0.9767 time: 0.0192s\n",
            "Epoch: 0168 loss_train: 0.2844 acc_train: 0.9433 loss_val: 0.2053 acc_val: 0.9733 time: 0.0173s\n",
            "Epoch: 0169 loss_train: 0.2892 acc_train: 0.9467 loss_val: 0.2054 acc_val: 0.9767 time: 0.0181s\n",
            "Epoch: 0170 loss_train: 0.2625 acc_train: 0.9600 loss_val: 0.2055 acc_val: 0.9767 time: 0.0244s\n",
            "Epoch: 0171 loss_train: 0.2669 acc_train: 0.9567 loss_val: 0.2056 acc_val: 0.9767 time: 0.0176s\n",
            "Epoch: 0172 loss_train: 0.2796 acc_train: 0.9567 loss_val: 0.2054 acc_val: 0.9767 time: 0.0186s\n",
            "Epoch: 0173 loss_train: 0.3270 acc_train: 0.9267 loss_val: 0.2054 acc_val: 0.9767 time: 0.0179s\n",
            "Epoch: 0174 loss_train: 0.2808 acc_train: 0.9367 loss_val: 0.2057 acc_val: 0.9733 time: 0.0216s\n",
            "Epoch: 0175 loss_train: 0.2811 acc_train: 0.9500 loss_val: 0.2059 acc_val: 0.9767 time: 0.0174s\n",
            "Epoch: 0176 loss_train: 0.2759 acc_train: 0.9733 loss_val: 0.2062 acc_val: 0.9767 time: 0.0229s\n",
            "Epoch: 0177 loss_train: 0.2662 acc_train: 0.9700 loss_val: 0.2064 acc_val: 0.9767 time: 0.0178s\n",
            "Epoch: 0178 loss_train: 0.2595 acc_train: 0.9467 loss_val: 0.2063 acc_val: 0.9767 time: 0.0177s\n",
            "Epoch: 0179 loss_train: 0.3157 acc_train: 0.9233 loss_val: 0.2061 acc_val: 0.9767 time: 0.0170s\n",
            "Epoch: 0180 loss_train: 0.2752 acc_train: 0.9667 loss_val: 0.2058 acc_val: 0.9767 time: 0.0192s\n",
            "Epoch: 0181 loss_train: 0.2778 acc_train: 0.9367 loss_val: 0.2059 acc_val: 0.9733 time: 0.0177s\n",
            "Epoch: 0182 loss_train: 0.2700 acc_train: 0.9533 loss_val: 0.2060 acc_val: 0.9767 time: 0.0175s\n",
            "Epoch: 0183 loss_train: 0.2690 acc_train: 0.9533 loss_val: 0.2062 acc_val: 0.9767 time: 0.0187s\n",
            "Epoch: 0184 loss_train: 0.2730 acc_train: 0.9567 loss_val: 0.2061 acc_val: 0.9767 time: 0.0238s\n",
            "Epoch: 0185 loss_train: 0.2764 acc_train: 0.9533 loss_val: 0.2060 acc_val: 0.9733 time: 0.0170s\n",
            "Epoch: 0186 loss_train: 0.2872 acc_train: 0.9533 loss_val: 0.2057 acc_val: 0.9733 time: 0.0179s\n",
            "Epoch: 0187 loss_train: 0.2889 acc_train: 0.9533 loss_val: 0.2050 acc_val: 0.9767 time: 0.0195s\n",
            "Epoch: 0188 loss_train: 0.2900 acc_train: 0.9400 loss_val: 0.2048 acc_val: 0.9800 time: 0.0178s\n",
            "Epoch: 0189 loss_train: 0.2918 acc_train: 0.9400 loss_val: 0.2053 acc_val: 0.9800 time: 0.0175s\n",
            "Epoch: 0190 loss_train: 0.2853 acc_train: 0.9433 loss_val: 0.2060 acc_val: 0.9733 time: 0.0205s\n",
            "Epoch: 0191 loss_train: 0.2708 acc_train: 0.9467 loss_val: 0.2061 acc_val: 0.9733 time: 0.0180s\n",
            "Epoch: 0192 loss_train: 0.2781 acc_train: 0.9467 loss_val: 0.2052 acc_val: 0.9767 time: 0.0175s\n",
            "Epoch: 0193 loss_train: 0.2824 acc_train: 0.9333 loss_val: 0.2046 acc_val: 0.9800 time: 0.0178s\n",
            "Epoch: 0194 loss_train: 0.2464 acc_train: 0.9733 loss_val: 0.2047 acc_val: 0.9733 time: 0.0235s\n",
            "Epoch: 0195 loss_train: 0.2813 acc_train: 0.9533 loss_val: 0.2056 acc_val: 0.9767 time: 0.0176s\n",
            "Epoch: 0196 loss_train: 0.3139 acc_train: 0.9533 loss_val: 0.2066 acc_val: 0.9733 time: 0.0176s\n",
            "Epoch: 0197 loss_train: 0.2646 acc_train: 0.9633 loss_val: 0.2072 acc_val: 0.9733 time: 0.0182s\n",
            "Epoch: 0198 loss_train: 0.2778 acc_train: 0.9533 loss_val: 0.2072 acc_val: 0.9733 time: 0.0173s\n",
            "Epoch: 0199 loss_train: 0.2627 acc_train: 0.9700 loss_val: 0.2067 acc_val: 0.9733 time: 0.0221s\n",
            "Epoch: 0200 loss_train: 0.2830 acc_train: 0.9500 loss_val: 0.2063 acc_val: 0.9733 time: 0.0174s\n",
            "Optimization Finished for 300 labeled nodes!\n",
            "Total time elapsed: 5.4025s\n",
            "Test set results: loss= 0.5217 accuracy= 0.8380\n"
          ]
        }
      ],
      "source": [
        "# from _future_ import division\n",
        "# from _future_ import print_function\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import os  # Added import for os module\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# from pygcn.utils import load_data, accuracy\n",
        "# from pygcn.models import GCN\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from layers import GraphConvolution\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
        "                    enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "\n",
        "def load_data(path=\"/content/cora/\", dataset=\"cora\"):\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
        "                                        dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
        "                                    dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize(features)\n",
        "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Set the training settings\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.no_cuda = False  # Set to True if you want to disable CUDA\n",
        "        self.fastmode = False\n",
        "        self.seed = 42\n",
        "        self.epochs = 200\n",
        "        self.lr = 0.01\n",
        "        self.weight_decay = 5e-4\n",
        "        self.hidden = 16\n",
        "        self.dropout = 0.5\n",
        "\n",
        "args = Args()\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "# Specify the data directory\n",
        "data_dir = '/content/cora/'\n",
        "\n",
        "# Load data\n",
        "adj, features, labels, _, idx_val, idx_test = load_data(data_dir)\n",
        "\n",
        "# Model and optimizer\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=args.hidden,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=args.dropout)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args.lr, weight_decay=args.weight_decay)\n",
        "\n",
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "\n",
        "# Define a list of labeled node counts\n",
        "labeled_nodes_counts = [60, 120, 180, 240, 300]\n",
        "\n",
        "for labeled_nodes_count in labeled_nodes_counts:\n",
        "    # Randomly choose labeled nodes\n",
        "    idx_train = np.random.choice(idx_val, labeled_nodes_count, replace=False)\n",
        "\n",
        "    def train(epoch):\n",
        "        t = time.time()\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(features, adj)\n",
        "        loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "        acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if not args.fastmode:\n",
        "            # Evaluate validation set performance separately,\n",
        "            # deactivates dropout during the validation run.\n",
        "            model.eval()\n",
        "            output = model(features, adj)\n",
        "\n",
        "        loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "        acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "        print('Epoch: {:04d}'.format(epoch + 1),\n",
        "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "              'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "              'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "              'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "    def test():\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "        loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "        acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "        print(\"Test set results:\",\n",
        "              \"loss= {:.4f}\".format(loss_test.item()),\n",
        "              \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "    # Train model\n",
        "    t_total = time.time()\n",
        "    for epoch in range(args.epochs):\n",
        "        train(epoch)\n",
        "    print(f\"Optimization Finished for {labeled_nodes_count} labeled nodes!\")\n",
        "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "    # Testing\n",
        "    test()"
      ]
    }
  ]
}