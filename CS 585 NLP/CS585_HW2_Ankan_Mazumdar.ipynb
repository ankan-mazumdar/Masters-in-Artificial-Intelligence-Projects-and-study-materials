{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d6aafbfa-2d87-4f34-91fc-6228d87954b9",
      "metadata": {
        "id": "d6aafbfa-2d87-4f34-91fc-6228d87954b9"
      },
      "source": [
        "# CS 585 - HW 2\n",
        "First, we will import packages and modules and load both the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PROBLEM 1 – Reading the data (5 pts)\n",
        "• Using Python, read in the 2 clickbait datasets (See section DATA), and combine both into a\n",
        "single, shuffled dataset. (One function to shuffle data is numpy.random.shuffle)\n",
        "• Next, split your dataset into train, test, and validation datasets. Use a split of 72% train, 8%\n",
        "validation, and 20% test. (Which is equivalent to a 20% test set, and the remainer split 90%/10%\n",
        "for train and validation).\n",
        "o If you prefer, you may save each split as an index (list of row numbers) rather than\n",
        "creating 3 separate datasets.\n",
        "• What is the \"target rate\" of each of these three datasets? That is, what % of the test dataset is\n",
        "labeled as clickbait? Show your result in your notebook."
      ],
      "metadata": {
        "id": "nCQDiE1-c55x"
      },
      "id": "nCQDiE1-c55x"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def read_and_format_text_file(filename):\n",
        "    # Read the text file with a newline delimiter\n",
        "    with open(filename, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Remove newline characters ('\\n') from each line\n",
        "    lines = [line.strip() for line in lines]\n",
        "\n",
        "    # Create a DataFrame with the specified column name\n",
        "    dataset = pd.DataFrame(lines)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "formatted_dataset1 = read_and_format_text_file('/content/clickbait.txt')\n",
        "formatted_dataset2 = read_and_format_text_file('/content/not-clickbait.txt')\n",
        "\n",
        "formatted_dataset1['label'] = 1\n",
        "\n",
        "formatted_dataset2['label'] = 0\n",
        "# print(formatted_dataset1.head(3))\n",
        "# print(formatted_dataset2.head(3))\n",
        "# print(formatted_dataset2.shape)\n",
        "combined_dataset = pd.concat([formatted_dataset1, formatted_dataset2])\n",
        "\n",
        "# print(combined_dataset.head(3))\n",
        "# Shuffle the combined dataset\n",
        "# shuffled_dataset = combined_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "# print (shuffled_dataset)\n",
        "\n",
        "combined_array = combined_dataset.values\n",
        "\n",
        "# Shuffle the NumPy array using numpy.random.shuffle\n",
        "np.random.shuffle(combined_array)\n",
        "\n",
        "# Convert the shuffled array back to a DataFrame\n",
        "shuffled_dataset = pd.DataFrame(combined_array)\n",
        "# print('type(shuffled_dataset)',type(shuffled_dataset))\n",
        "\n",
        "# print(shuffled_dataset.tail(100))\n",
        "# shuffled_dataset.rename(columns={0: 'text', 1: 'label'}, inplace=True)\n",
        "shuffled_dataset = shuffled_dataset.rename(columns={0: 'text', 1: 'label'})\n",
        "\n",
        "# print(shuffled_dataset.head(3))\n",
        "count_rows_with_label_1 = shuffled_dataset['label'].sum()\n",
        "# print('shuffled_dataset with count',shuffled_dataset)\n",
        "# Define the split percentages\n",
        "train_split = 0.72\n",
        "validation_split = 0.08\n",
        "test_split = 0.20\n",
        "# test_size = len(shuffled_dataset) - train_split - validation_split\n",
        "\n",
        "# Calculate the number of samples for each split\n",
        "total_samples = len(shuffled_dataset)\n",
        "train_samples = int(train_split * total_samples)\n",
        "validation_samples = int(validation_split * total_samples)\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "\n",
        "\n",
        "train_data = shuffled_dataset.iloc[:train_samples].copy()\n",
        "validation_data = shuffled_dataset.iloc[train_samples:train_samples+validation_samples].copy()\n",
        "test_data = shuffled_dataset.iloc[train_samples+validation_samples:].copy()\n",
        "\n",
        "\n",
        "# print('train_data'train_data.shape)\n",
        "# print(validation_data.shape)\n",
        "# print(test_data.shape)\n",
        "# test_data\n",
        "\n",
        "#clickbait precentage calculator\n",
        "def calculate_clickbait_percentage(dataset):\n",
        "    # Clean the 'label' column by stripping whitespace and converting to string.\n",
        "    dataset['label'] = dataset['label'].astype(str).str.strip()\n",
        "\n",
        "    # Calculate the count of rows with the label '1'.\n",
        "    count_rows_with_label_1 = (dataset['label'] == '1').sum()\n",
        "\n",
        "    # Calculate the total number of rows in the dataset.\n",
        "    total_num_rows = len(dataset)\n",
        "\n",
        "    # Calculate the percentage of rows labeled as '1'.\n",
        "    percentage = (count_rows_with_label_1 / total_num_rows) * 100\n",
        "\n",
        "    return percentage\n",
        "\n",
        "# Usage example:\n",
        "# Assuming your DataFrame is named 'test_data'\n",
        "train_clickbait_percentage = calculate_clickbait_percentage(train_data)\n",
        "validation_clickbait_percentage = calculate_clickbait_percentage(validation_data)\n",
        "test_clickbait_percentage = calculate_clickbait_percentage(test_data)\n",
        "print(f\"Percentage of clickbait in the train dataset: {train_clickbait_percentage:.2f}%\")\n",
        "print(f\"Percentage of clickbait in the validation  dataset: {validation_clickbait_percentage:.2f}%\")\n",
        "print(f\"Percentage of clickbait in the test dataset: {test_clickbait_percentage:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9q6BGELtqXg",
        "outputId": "5bdc301c-736e-4c5d-b276-4800e7bd76bd"
      },
      "id": "R9q6BGELtqXg",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of clickbait in the train dataset: 34.50%\n",
            "Percentage of clickbait in the validation  dataset: 37.70%\n",
            "Percentage of clickbait in the test dataset: 31.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PROBLEM 3 – Training a single Bag-of-Words (BOW) Text Classifier (20 pts)\n",
        "• Using scikit-learn pipelines module, create a Pipeline to train a BOW naïve bayes model. We\n",
        "suggest the classes CountVectorizer and MultinomialNB. Include both unigrams and bigrams in\n",
        "your model in your vectorizer vocabulary (see parameter: ngram_range)\n",
        "• Fit your classifier on your training set\n",
        "• Compute the precision, recall, and F1-score on both your training and validation datasets using\n",
        "functions in sklearn.metrics. Show results in your notebook. Use \"clickbait\" is your target class\n",
        "(I.e., y=1 for clickbait and y=0 for non-clickbait)\n",
        "ALTERNATIVE: If you are already well-versed in Naïve Bayes, you may select another bag-of-words\n",
        "classifier for this problem. Your method should have some way to select top features or key indicators,\n",
        "mapped to words or n-grams in your vocabulary, so that you can complete the remaining problems"
      ],
      "metadata": {
        "id": "XN0sqz5Do5zW"
      },
      "id": "XN0sqz5Do5zW"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Create a pipeline with CountVectorizer and MultinomialNB\n",
        "text_classification_pipeline = Pipeline([\n",
        "    ('text_vectorizer', CountVectorizer(ngram_range=(1, 2))),  # Include unigrams and bigrams\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Fit the classifier on your training set\n",
        "train_data = train_data.copy()\n",
        "validation_data = validation_data.copy()\n",
        "train_data['label'] = train_data['label'].astype(int)\n",
        "validation_data['label'] = validation_data['label'].astype(int)\n",
        "\n",
        "text_classification_pipeline.fit(train_data['text'], train_data['label'])\n",
        "\n",
        "# Predictions on training and validation sets\n",
        "train_predictions = text_classification_pipeline.predict(train_data['text'])\n",
        "valid_predictions = text_classification_pipeline.predict(validation_data['text'])\n",
        "\n",
        "# Compute precision, recall, and F1-score on the training set\n",
        "train_precision = precision_score(train_data['label'], train_predictions, average='binary', pos_label=1)\n",
        "train_recall = recall_score(train_data['label'], train_predictions, average='binary', pos_label=1)\n",
        "train_f1_score = f1_score(train_data['label'], train_predictions, average='binary', pos_label=1)\n",
        "\n",
        "# Compute precision, recall, and F1-score on the validation set\n",
        "valid_precision = precision_score(validation_data['label'], valid_predictions, average='binary', pos_label=1)\n",
        "valid_recall = recall_score(validation_data['label'], valid_predictions, average='binary', pos_label=1)\n",
        "valid_f1_score = f1_score(validation_data['label'], valid_predictions, average='binary', pos_label=1)\n",
        "\n",
        "# Display results\n",
        "print(f\"Train data Precision: {train_precision:.4f}\")\n",
        "print(f\"Train data Recall: {train_recall:.4f}\")\n",
        "print(f\"Train data F1-Score: {train_f1_score:.4f}\")\n",
        "print(f\"Validation data Precision: {valid_precision:.4f}\")\n",
        "print(f\"Validation data Recall: {valid_recall:.4f}\")\n",
        "print(f\"Validation data F1-Score: {valid_f1_score:.4f}\")\n",
        "\n",
        "# Check if we can exclude stop words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIpkJGO1VS6d",
        "outputId": "1176a121-2b05-4319-f853-0e35f5b24198"
      },
      "id": "GIpkJGO1VS6d",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data Precision: 0.9900\n",
            "Train data Recall: 0.9983\n",
            "Train data F1-Score: 0.9941\n",
            "Validation data Precision: 0.9104\n",
            "Validation data Recall: 0.8472\n",
            "Validation data F1-Score: 0.8777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PROBLEM 4 – Hyperparameter Tuning (20 pts)\n",
        "Using the ParameterGrid class, run a small grid search where you vary at least 3 parameters of your\n",
        "model\n",
        "• max_df for your count vectorizer (threshold to filter document frequency)\n",
        "• alpha or smoothing of your NaïveBayes model\n",
        "• One other parameter of your choice. This can be non-numeric; for example, you can consider a\n",
        "model with and without bigrams (see parameter \"ngram\" in class CountVectorizer)\n",
        "Show metrics on your validation set for precision, recall, and F1-score. If your grid search is very large\n",
        "(>50 rows) you may limit output to the highest and lowest results.\n",
        "\n",
        "ALTERNATIVE – If you used a method other than Naïve Bayes in Problem 3, then be sure it is clear what\n",
        "metrics you tuned in Problem 4."
      ],
      "metadata": {
        "id": "0i3T03DophvJ"
      },
      "id": "0i3T03DophvJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution for Paramater GRID-"
      ],
      "metadata": {
        "id": "hKCH6NB8oUPI"
      },
      "id": "hKCH6NB8oUPI"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download NLTK stopwords (if not already downloaded)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define a function to remove stopwords using NLTK\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Assuming 'train_data' and 'validation_data' are your datasets\n",
        "train_data = train_data.copy()\n",
        "validation_data = validation_data.copy()\n",
        "train_data['cleaned_text'] = train_data['text'].apply(remove_stopwords)\n",
        "validation_data['cleaned_text'] = validation_data['text'].apply(remove_stopwords)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'vectorizer__max_df': [0.2, 0.4, 0.6],\n",
        "    'classifier__alpha': [0.1, 1.0, 3.0 , 5.0],\n",
        "    'vectorizer__ngram_range': [(1, 1), (1, 2)]  # Varying ngram range\n",
        "}\n",
        "\n",
        "# Initialize an empty list to store results\n",
        "results_list = []\n",
        "\n",
        "# Create a pipeline with CountVectorizer and MultinomialNB\n",
        "text_classification_pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Perform grid search and compute metrics for each parameter combination\n",
        "for params in ParameterGrid(param_grid):\n",
        "    # Set parameters for the pipeline\n",
        "    text_classification_pipeline.set_params(**params)\n",
        "\n",
        "    # Fit the pipeline on the training data\n",
        "    text_classification_pipeline.fit(train_data['cleaned_text'], train_data['label'])\n",
        "\n",
        "    # Make predictions on the validation data\n",
        "    validation_predictions = text_classification_pipeline.predict(validation_data['cleaned_text'])\n",
        "\n",
        "    # Compute precision, recall, and F1-score\n",
        "    precision = precision_score(validation_data['label'], validation_predictions)\n",
        "    recall = recall_score(validation_data['label'], validation_predictions)\n",
        "    f1 = f1_score(validation_data['label'], validation_predictions)\n",
        "\n",
        "    # Append results to the list\n",
        "    results_list.append({\n",
        "        'max_df': params['vectorizer__max_df'],\n",
        "        'alpha': params['classifier__alpha'],\n",
        "        'ngram_range': params['vectorizer__ngram_range'],\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    })\n",
        "\n",
        "# Create a DataFrame by concatenating the results list\n",
        "results_df = pd.concat([pd.DataFrame(result) for result in results_list], ignore_index=True)\n",
        "\n",
        "\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHRSeOkrc4Dk",
        "outputId": "f6fc8fe4-156b-4cc0-bd79-7afd076b3efd"
      },
      "id": "kHRSeOkrc4Dk",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    max_df  alpha  ngram_range  precision    recall  f1_score\n",
            "0      0.2    0.1            1   0.805970  0.750000  0.776978\n",
            "1      0.2    0.1            1   0.805970  0.750000  0.776978\n",
            "2      0.2    0.1            1   0.833333  0.763889  0.797101\n",
            "3      0.2    0.1            2   0.833333  0.763889  0.797101\n",
            "4      0.4    0.1            1   0.805970  0.750000  0.776978\n",
            "5      0.4    0.1            1   0.805970  0.750000  0.776978\n",
            "6      0.4    0.1            1   0.833333  0.763889  0.797101\n",
            "7      0.4    0.1            2   0.833333  0.763889  0.797101\n",
            "8      0.6    0.1            1   0.805970  0.750000  0.776978\n",
            "9      0.6    0.1            1   0.805970  0.750000  0.776978\n",
            "10     0.6    0.1            1   0.833333  0.763889  0.797101\n",
            "11     0.6    0.1            2   0.833333  0.763889  0.797101\n",
            "12     0.2    1.0            1   0.830769  0.750000  0.788321\n",
            "13     0.2    1.0            1   0.830769  0.750000  0.788321\n",
            "14     0.2    1.0            1   0.836066  0.708333  0.766917\n",
            "15     0.2    1.0            2   0.836066  0.708333  0.766917\n",
            "16     0.4    1.0            1   0.830769  0.750000  0.788321\n",
            "17     0.4    1.0            1   0.830769  0.750000  0.788321\n",
            "18     0.4    1.0            1   0.836066  0.708333  0.766917\n",
            "19     0.4    1.0            2   0.836066  0.708333  0.766917\n",
            "20     0.6    1.0            1   0.830769  0.750000  0.788321\n",
            "21     0.6    1.0            1   0.830769  0.750000  0.788321\n",
            "22     0.6    1.0            1   0.836066  0.708333  0.766917\n",
            "23     0.6    1.0            2   0.836066  0.708333  0.766917\n",
            "24     0.2    3.0            1   0.859649  0.680556  0.759690\n",
            "25     0.2    3.0            1   0.859649  0.680556  0.759690\n",
            "26     0.2    3.0            1   0.865385  0.625000  0.725806\n",
            "27     0.2    3.0            2   0.865385  0.625000  0.725806\n",
            "28     0.4    3.0            1   0.859649  0.680556  0.759690\n",
            "29     0.4    3.0            1   0.859649  0.680556  0.759690\n",
            "30     0.4    3.0            1   0.865385  0.625000  0.725806\n",
            "31     0.4    3.0            2   0.865385  0.625000  0.725806\n",
            "32     0.6    3.0            1   0.859649  0.680556  0.759690\n",
            "33     0.6    3.0            1   0.859649  0.680556  0.759690\n",
            "34     0.6    3.0            1   0.865385  0.625000  0.725806\n",
            "35     0.6    3.0            2   0.865385  0.625000  0.725806\n",
            "36     0.2    5.0            1   0.900000  0.625000  0.737705\n",
            "37     0.2    5.0            1   0.900000  0.625000  0.737705\n",
            "38     0.2    5.0            1   0.956522  0.611111  0.745763\n",
            "39     0.2    5.0            2   0.956522  0.611111  0.745763\n",
            "40     0.4    5.0            1   0.900000  0.625000  0.737705\n",
            "41     0.4    5.0            1   0.900000  0.625000  0.737705\n",
            "42     0.4    5.0            1   0.956522  0.611111  0.745763\n",
            "43     0.4    5.0            2   0.956522  0.611111  0.745763\n",
            "44     0.6    5.0            1   0.900000  0.625000  0.737705\n",
            "45     0.6    5.0            1   0.900000  0.625000  0.737705\n",
            "46     0.6    5.0            1   0.956522  0.611111  0.745763\n",
            "47     0.6    5.0            2   0.956522  0.611111  0.745763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution using Grid CV-"
      ],
      "metadata": {
        "id": "Ead4dMayoAgy"
      },
      "id": "Ead4dMayoAgy"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords (if not already downloaded)\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define a function to remove stopwords using NLTK\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Assuming 'train_data' is your dataset\n",
        "train_data = train_data.copy()\n",
        "train_data['text'] = train_data['text'].apply(remove_stopwords)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'vectorizer__max_df': [0.2, 0.4, 0.6],\n",
        "    'classifier__alpha': [0.1, 1.0, 3.0 , 5.0],\n",
        "    'vectorizer__ngram_range': [(1, 1), (1, 2)]  # Varying ngram range\n",
        "}\n",
        "\n",
        "# Create a pipeline with CountVectorizer and MultinomialNB\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search_cv = GridSearchCV(\n",
        "    pipeline, param_grid, cv=5, scoring='f1_macro', verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit the GridSearchCV object on the training data\n",
        "grid_search_cv.fit(train_data['text'], train_data['label'])\n",
        "\n",
        "# Print the best parameters and corresponding metrics\n",
        "print(\"Best Parameters: \", grid_search_cv.best_params_)\n",
        "print(\"Best F1 Score: \", grid_search_cv.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZA0XeQqdTEh",
        "outputId": "f7a9b609-ff02-4d02-8a45-2c05b3a0ec3e"
      },
      "id": "MZA0XeQqdTEh",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "Best Parameters:  {'classifier__alpha': 1.0, 'vectorizer__max_df': 0.2, 'vectorizer__ngram_range': (1, 2)}\n",
            "Best F1 Score:  0.8366997663191809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As F1 score from Grid CV is better than parameter grid, we will go ahead with GRID CV results"
      ],
      "metadata": {
        "id": "FBieQBo3o0qH"
      },
      "id": "FBieQBo3o0qH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PROBLEM 5 – Model selection (10pts)\n",
        "Using these validation-set metrics from the previous problem, choose one model as your selected\n",
        "model. It is up to you how to choose this model; one approach is to choose the model that shows the\n",
        "highest F1-score on your training set.\n",
        "Next apply your selected model to your test set and compute precision, recall and F1. Show results in\n",
        "your notebook"
      ],
      "metadata": {
        "id": "QGiO-P5Mp4RG"
      },
      "id": "QGiO-P5Mp4RG"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# GridSearchCV and obtained the best model\n",
        "selected_model = grid_search_cv.best_estimator_\n",
        "\n",
        "# Apply the selected model to the test set\n",
        "test_data = test_data.copy()\n",
        "test_data['label'] = test_data['label'].astype(int)\n",
        "test_predictions = selected_model.predict(test_data['text'])\n",
        "\n",
        "# Calculate precision, recall, and F1-score on the test set\n",
        "test_precision = precision_score(test_data['label'], test_predictions, average='binary', pos_label=1)\n",
        "test_recall = recall_score(test_data['label'], test_predictions, average='binary')\n",
        "test_f1_score = f1_score(test_data['label'], test_predictions, average='binary')\n",
        "\n",
        "# Print the test metrics\n",
        "print(f\"Test data Precision: {test_precision:.4f}\")\n",
        "print(f\"Test data Recall: {test_recall:.4f}\")\n",
        "print(f\"Test F1 Score: {test_f1_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "GTz1dGX4tpro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867bf54d-5a1b-490f-9e0c-af4b97a90b1d"
      },
      "id": "GTz1dGX4tpro",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test data Precision: 0.8344\n",
            "Test data Recall: 0.8792\n",
            "Test F1 Score: 0.8562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PROBLEM 6 – Key Indicators (10pts)\n",
        "Using the log-probabilities of the model you selected in the previous problem, select 5 words that are\n",
        "strong Clickbait indicators. That is, if you needed to filter headlines based on a single word, without a\n",
        "machine learning model, then these words would be good options. Show this list of keywords in your\n",
        "notebook.\n",
        "You can choose how to handle bigrams (e.g., \"win<space>big\"); you may choose to ignore them and\n",
        "only select unigram vocabulary words as key indicators."
      ],
      "metadata": {
        "id": "cMkLnb2OaJOz"
      },
      "id": "cMkLnb2OaJOz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Access feature log probabilities from the MultinomialNB model\n",
        "feature_log_probs = selected_model.named_steps['classifier'].feature_log_prob_\n",
        "\n",
        "#  CountVectorizer named as 'countvectorizer'\n",
        "feature_names = selected_model.named_steps['vectorizer'].get_feature_names_out()\n",
        "\n",
        "# Create a dictionary to store feature names and their associated log probabilities, feature_log_probs[1][i] will access elements indexed as label 1 that is clickbait.\n",
        "feature_probs_dict = {feature_names[i]: feature_log_probs[1][i] for i in range(len(feature_names))}\n",
        "\n",
        "# Sort the dictionary by log probabilities to get the most informative features\n",
        "sorted_features_items = sorted(feature_probs_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Display the top 5 informative features\n",
        "top_features = sorted_features_items[:5]\n",
        "print('top_features along with their log probabilities',top_features)\n",
        "\n",
        "top_words = [item[0] for item in sorted_features_items[:5]]\n",
        "print('5 top Clickbait indicating words -',top_words)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSdBxug2Omdh",
        "outputId": "61b5673e-4547-4086-d469-b6f2309bbf4c"
      },
      "id": "NSdBxug2Omdh",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top_features along with their log probabilities [('believe', -6.213260251709267), ('one', -6.276439153330799), ('never', -6.578720025203732), ('here', -6.608572988353414), ('new', -6.846984011798412)]\n",
            "5 top Clickbait indicating words - ['believe', 'one', 'never', 'here', 'new']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PROBLEM 7 – Regular expressions (10pts)\n",
        "Your IT department has reached out to you because they heard you can help them find clickbait. They\n",
        "are interested in your machine learning model, but they need a solution today.\n",
        "• Write a regular expression that checks if any of the keywords from the previous problem are\n",
        "found in the text. You should write one regular expression that detects any of your top 5\n",
        "keywords. Your regular expression should be aware of word boundaries in some way. That is,\n",
        "the keyword \"win\" should not be detected in the text \"Gas prices up in winter months\".\n",
        "• Using the python re library – apply your function to your test set. (See function re.search). What\n",
        "is the precision and recall of this classifier? Show your results in your notebook"
      ],
      "metadata": {
        "id": "NsX51GUnaM8b"
      },
      "id": "NsX51GUnaM8b"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Define the regular expression pattern\n",
        "# pattern = r'\\b(?:' + '|'.join(top_words) + r')\\b'\n",
        "\n",
        "pattern = r'\\b(?:' + '|'.join(re.escape(keyword) for keyword in top_words) + r')\\b'\n",
        "\n",
        "# function to classify the text is based on  pattern\n",
        "def clickbait_dataset_with_regex_classifier(text):\n",
        "    return bool(re.search(pattern, text, flags=re.IGNORECASE))\n",
        "\n",
        "#  function to the test dataset\n",
        "test_data = test_data.copy()\n",
        "test_data.loc[:, 'clickbait_predict'] = test_data['text'].apply(clickbait_dataset_with_regex_classifier)\n",
        "\n",
        "# Calculat the precision and recall for test dataset\n",
        "tp = len(test_data[(test_data['clickbait_predict'] == 1) & (test_data['label'] == 1)])\n",
        "fp = len(test_data[(test_data['clickbait_predict'] == 1) & (test_data['label'] == 0)])\n",
        "fn = len(test_data[(test_data['clickbait_predict'] == 0) & (test_data['label'] == 1)])\n",
        "\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8Dcs8OHtbQ_",
        "outputId": "ae7173bf-91a2-4e1f-e6aa-d7dc040e2c27"
      },
      "id": "b8Dcs8OHtbQ_",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.7333\n",
            "Recall: 0.2953\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}