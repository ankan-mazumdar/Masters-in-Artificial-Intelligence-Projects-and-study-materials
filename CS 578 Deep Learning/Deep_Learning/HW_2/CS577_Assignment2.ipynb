{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7_hx8SJJR4-"
   },
   "source": [
    "# Assignment 2 - Recurrent Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AiWTVDf7cZ2"
   },
   "source": [
    "## Programming (Full points: 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, our goal is to use PyTorch to implement Recurrent Neural Networks (RNN) for sentiment analysis task. Sentiment analysis is to classify sentences (input) into certain sentiments (output labels), which includes positive, negative and neutral.\n",
    "\n",
    "We will use a benckmark dataset, SST, for this assignment.\n",
    "* we download the SST dataset from torchtext package, and do some preprocessing to build vocabulary and split the dataset into training/validation/test sets. You don't need to modify the code in this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "# load data splits\n",
    "train_data, val_data, test_data = datasets.SST.splits(TEXT, LABEL)\n",
    "\n",
    "# build dictionary\n",
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# hyperparameters\n",
    "vocab_size = len(TEXT.vocab)\n",
    "label_size = len(LABEL.vocab)\n",
    "padding_idx = TEXT.vocab.stoi['<pad>']\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "\n",
    "# build iterators\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train_data, val_data, test_data), \n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* define the training and evaluation function in the cell below.\n",
    "### (25 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, labels = batch.text, batch.label\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch.text, batch.label\n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* build a RNN model for sentiment analysis in the cell below.\n",
    "We have provided several hyperparameters we needed for building the model, including vocabulary size (vocab_size), the word embedding dimension (embedding_dim), the hidden layer dimension (hidden_dim), the number of layers (num_layers) and the number of sentence labels (label_size). Please fill in the missing codes, and implement a RNN model.\n",
    "### (40 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, label_size, padding_idx):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, label_size)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text shape: (batch_size, seq_length)\n",
    "        \n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # RNN layer\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        # Get the output from the last time step\n",
    "        last_output = output[:, -1, :]\n",
    "        \n",
    "        # Fully connected layer\n",
    "        predictions = self.fc(last_output)\n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train the model and compute the accuracy in the cell below.\n",
    "### (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Training Loss: 1.0532 | Training Accuracy: 40.80%\n",
      "Validation Loss: 1.0658\n",
      "Epoch 2/10:\n",
      "Training Loss: 1.0492 | Training Accuracy: 41.37%\n",
      "Validation Loss: 1.1411\n",
      "Epoch 3/10:\n",
      "Training Loss: 1.0462 | Training Accuracy: 41.68%\n",
      "Validation Loss: 1.0740\n",
      "Epoch 4/10:\n",
      "Training Loss: 1.0460 | Training Accuracy: 42.17%\n",
      "Validation Loss: 1.1246\n",
      "Epoch 5/10:\n",
      "Training Loss: 1.0457 | Training Accuracy: 42.59%\n",
      "Validation Loss: 1.1068\n",
      "Epoch 6/10:\n",
      "Training Loss: 1.0430 | Training Accuracy: 41.94%\n",
      "Validation Loss: 1.0902\n",
      "Epoch 7/10:\n",
      "Training Loss: 1.0419 | Training Accuracy: 42.42%\n",
      "Validation Loss: 1.1478\n",
      "Epoch 8/10:\n",
      "Training Loss: 1.0369 | Training Accuracy: 42.52%\n",
      "Validation Loss: 1.2493\n",
      "Epoch 9/10:\n",
      "Training Loss: 1.0357 | Training Accuracy: 42.44%\n",
      "Validation Loss: 1.2372\n",
      "Epoch 10/10:\n",
      "Training Loss: 1.0366 | Training Accuracy: 42.44%\n",
      "Validation Loss: 1.1913\n",
      "Test Loss: 1.2089\n",
      "Test Accuracy: 39.55%\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "vocab_size = len(TEXT.vocab)\n",
    "label_size = len(LABEL.vocab)\n",
    "padding_idx = TEXT.vocab.stoi['<pad>']\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "num_layers = 1\n",
    "num_epochs = 10  # You can adjust the number of epochs\n",
    "\n",
    "# Define the RNN model\n",
    "model = RNNModel(vocab_size, embedding_dim, hidden_dim, num_layers, label_size, padding_idx)\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in train_iter:\n",
    "        optimizer.zero_grad()\n",
    "        text, labels = batch.text, batch.label\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Compute accuracy\n",
    "        predicted_labels = torch.argmax(predictions, dim=1)\n",
    "        correct += (predicted_labels == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = epoch_loss / len(train_iter)\n",
    "    train_accuracy = correct / total\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    val_loss = evaluate(model, val_iter, criterion)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'Training Loss: {train_loss:.4f} | Training Accuracy: {train_accuracy*100:.2f}%')\n",
    "    print(f'Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss = evaluate(model, test_iter, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# Function to compute accuracy on the test set\n",
    "def compute_accuracy(model, iterator):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch.text, batch.label\n",
    "            predictions = model(text)\n",
    "            predicted_labels = torch.argmax(predictions, dim=1)\n",
    "            correct += (predicted_labels == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Calculate and print test accuracy\n",
    "test_accuracy = compute_accuracy(model, test_iter)\n",
    "print(f'Test Accuracy: {test_accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* try to train a model with better accuracy in the cell below. For example, you can use different optimizers such as SGD and Adam. You can also compare different hyperparameters and model size.\n",
    "### (15 points), to obtain FULL point in this problem, the accuracy needs to be higher than 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Iterator.__init__() got an unexpected keyword argument 'max_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# build iterators\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m train_iter, val_iter, test_iter \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBucketIterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[0;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRNNClassifier\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchtext\\data\\iterator.py:95\u001b[0m, in \u001b[0;36mIterator.splits\u001b[1;34m(cls, datasets, batch_sizes, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(datasets)):\n\u001b[0;32m     94\u001b[0m     train \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 95\u001b[0m     ret\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(ret)\n",
      "\u001b[1;31mTypeError\u001b[0m: Iterator.__init__() got an unexpected keyword argument 'max_len'"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "LABEL= data.LabelField()\n",
    "\n",
    "#load data splits\n",
    "train_data, val_data, test_data = datasets.SST.splits (TEXT, LABEL)\n",
    "\n",
    "# build dictionary\n",
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "#hyperparameters\n",
    "vocab_size = len(TEXT.vocab)\n",
    "label_size = len(LABEL.vocab)\n",
    "padding_idx= TEXT.vocab.stoi['<pad>']\n",
    "embedding_dim= 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "\n",
    "# build iterators\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train_data, val_data, test_data),\n",
    "    batch_size=32,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    max_len=50\n",
    ")\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers=1):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.label_size = label_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # add the layers required for sentiment analysis.\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=padding_idx)\n",
    "        self.rnn = nn.LSTM(self.embedding_dim, self.hidden_dim, num_layers=self.num_layers, bidirectional=True)\n",
    "        self.linear = nn.Linear(self.hidden_dim * 2, self.label_size)\n",
    "\n",
    "    def zero_state(self, batch_size): \n",
    "        # implement the function, which returns an initial hidden state.\n",
    "        hidden_state = torch.zeros((self.num_layers * 2, batch_size, self.hidden_dim))\n",
    "        cell_state = torch.zeros((self.num_layers * 2, batch_size, self.hidden_dim))\n",
    "        return hidden_state, cell_state\n",
    "\n",
    "    def forward(self, text):\n",
    "        # implement the forward function of the model.\n",
    "        embedding = self.embedding(text)\n",
    "\n",
    "        # initialize the hidden state\n",
    "        hidden_state, cell_state = self.zero_state(embedding.size(0))\n",
    "\n",
    "        # pass the embedding through the RNN\n",
    "        outputs, (hidden_state, cell_state) = self.rnn(embedding, (hidden_state, cell_state))\n",
    "\n",
    "        # take the last hidden state and pass it through a linear layer\n",
    "        last_hidden_state = hidden_state[-1]\n",
    "        logits = self.linear(last_hidden_state)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# create the model\n",
    "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers)\n",
    "\n",
    "# define the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
